---
title: LambdaMartç®—æ³•åŸç†ä¸å®ç°
date: 2021-12-11 16:31:59
description: LambdaMartç®—æ³•ä½œä¸ºç»å…¸çš„æœç´¢æ’åºç®—æ³•ï¼Œè‡³ä»Šä¾ç„¶è¢«å„å¤§äº’è”ç½‘å…¬å¸ä½¿ç”¨ï¼Œåœ¨å·¥ä½œä¸­ä½¿ç”¨åˆ°è¯¥ç®—æ³•ï¼Œè¿™é‡Œè¿›è¡Œè¯¦ç»†å­¦ä¹ ã€‚
tags:
    æ’åº
categories:
    æœºå™¨å­¦ä¹ 
mathjax:
    true
---

# NDCGè¯„ä¼°æŒ‡æ ‡

NDCGç”¨äºè¯„ä¼°æœç´¢è¿”å›çš„ç»“æœç›¸å…³æ€§æŒ‡æ ‡ã€‚å…¶ç”±CGè½¬æ¢åˆ°DCGå†å‘å±•å‡ºnDCGã€‚ä¸‹é¢ä¾æ¬¡ä»‹ç»ã€‚

## ç´¯è®¡å¢ç›Š(CG)

CGï¼Œcumulative gainï¼Œæ˜¯DCGçš„å‰èº«ï¼Œåªè€ƒè™‘åˆ°äº†ç›¸å…³æ€§çš„å…³è”ç¨‹åº¦ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°ä½ç½®çš„å› ç´ ã€‚å®ƒæ˜¯ä¸€ä¸ªæœç´ ç»“æœç›¸å…³æ€§åˆ†æ•°çš„æ€»å’Œã€‚æŒ‡å®šä½ç½®Pä¸Šçš„CGä¸ºï¼š
$$
CG_P=\sum_{i=1}^P rel_i
$$
å…¶ä¸­$rel_i$ä¸ºç¬¬$i$ä¸ªä½ç½®ä¸Šçš„æ–‡æ¡£å’Œqueryçš„ç›¸å…³æ€§ã€‚

è¯¥åº¦é‡æ–¹æ³•å­˜åœ¨é—®é¢˜ï¼Œå› ä¸ºå…¶æ— æ³•åŒºåˆ†æ’åºé¡ºåºï¼Œåªèƒ½åˆ¤æ–­å¬å›çš„æ•°æ®æ˜¯å¦ç›¸å…³ï¼Œæ¯”å¦‚ä¸€ä¸ªqueryåº”è¯¥å¬å›ä¸‰ä¸ªæ–‡æ¡£$a,b,c$ã€‚å…¶ç›¸å…³æ€§ä¾æ¬¡é™ä½ï¼Œä½†å¦‚æœå¬å›çš„å®é™…é¡ºåºæ˜¯$c,b,a$æ—¶ï¼Œè¯¥è¯„ä¼°æŒ‡æ ‡å’Œæœ€ä¼˜æƒ…å†µä¸‹çš„æŒ‡æ ‡å€¼æ˜¯ä¸€æ ·çš„ã€‚

ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå¼•å…¥äº†DCGã€‚

## æŠ˜æŸç´¯è®¡å¢ç›Š(DCG)

ç›¸æ¯”äºCGï¼ŒDCGåœ¨CGçš„æ¯ä¸€ä¸ªç»“æœä¸Šå¢åŠ äº†ä¸€ä¸ªæŠ˜æŸå€¼ã€‚ç›®çš„å°±æ˜¯ä¸ºäº†è®©æ’åè¶Šé å‰çš„ç»“æœè¶Šèƒ½å½±å“æœ€åçš„ç»“æœã€‚å‡è®¾æ’åºè¶Šå¾€åï¼Œä»·å€¼è¶Šä½ã€‚åˆ°ç¬¬iä¸ªä½ç½®çš„æ—¶å€™ï¼Œå®ƒçš„ä»·å€¼æ˜¯ $\frac{1}{log_2{(i+1)}}$ï¼Œé‚£ä¹ˆç¬¬iä¸ªç»“æœäº§ç”Ÿçš„æ•ˆç›Šå°±æ˜¯ $rel_i * \frac{1}{log_2{(i+1)}}$ï¼Œå› æ­¤ï¼š
$$
DCG_P\sum_i^P\frac{rel_i}{log_2{(i+1)}}
$$
å› ä¸ºæ˜¯ä¹˜ä»¥ä¸€ä¸ªæŠ˜æŸå€¼ï¼Œå› æ­¤è¯¥å€¼å¯ä»¥å–åˆ«çš„å‡½æ•°ï¼Œè¿˜æœ‰ä¸€ä¸ªæ›´åŠ å¸¸ç”¨çš„å…¬å¼ï¼Œå¦‚ä¸‹ï¼š
$$
DCG_P\sum_i^P\frac{2^{rel_i}-1}{log_2{(i+1)}}
$$
DCGè§£å†³äº†åŒä¸€ä¸ªqueryä¸‹æ’åºçš„æŒ‡æ ‡ï¼Œä½†æ˜¯å¯¹äºä¸åŒqueryå…¶å¬å›çš„æ–‡æ¡£æ•°é‡æ˜¯ä¸ä¸€è‡´çš„ï¼ŒDCGé‡‡ç”¨çš„æ˜¯ç´¯åŠ çš„æ–¹å¼ï¼Œè¿™å¯¹åº”å¬å›æ–‡æ¡£æ›´å¤šçš„queryæ˜¯æ›´åŠ å‹å¥½çš„ï¼Œä½†ä¸ç¬¦åˆå®é™…ä½¿ç”¨ï¼Œå› æ­¤NDCGè¢«æå‡ºä»¥æ”¹è¿›DCGã€‚

## å½’ä¸€åŒ–æŠ˜æŸç´¯è®¡å¢ç›Š(NDCG)

ç”±äºDCGæ˜¯ä¸€ä¸ªç´¯åŠ å€¼ï¼Œæ— æ³•å¯¹ä¸åŒæœç´¢ç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œå› æ­¤NDCCå¯¹æ¯ä¸ªqueryçš„ç»“æœè¿›è¡Œäº†å½’ä¸€åŒ–æ“ä½œï¼Œå…¶æ–¹æ³•æ˜¯å¯¹DCGå€¼é™¤ä»¥IDCG(ç†æƒ³æƒ…å†µä¸‹æœ€å¤§DCGå€¼)ï¼š
$$
nDCG_P=\frac{DCG_p}{IDCG_p}
$$

$$
IDCG_P = \sum_{i=1}^{\vert RET \vert}\frac{2^{rel_i}-1}{log_2{(i+1)}}
$$

å…¶ä¸­$\vert REL \vert$è¡¨ç¤ºï¼Œç»“æœæŒ‰ç…§ç›¸å…³æ€§ä»å¤§åˆ°å°çš„é¡ºåºæ’åºï¼Œå–å‰Pä¸ªç»“æ„ç»„åˆæˆçš„é›†åˆï¼Œå³æŒ‰ç…§æœ€ä¼˜çš„æ–¹å¼å¯¹ç»“æœè¿›è¡Œæ’åºæ˜¯çš„DCGå€¼ã€‚

# æ’åºå­¦ä¹ LTR

**æ’åºå­¦ä¹ ï¼ˆLearning to Rankï¼ŒLTRï¼‰**ï¼Œä¹Ÿç§°**æœºå™¨æ’åºå­¦ä¹ ï¼ˆMachine-learned Rankingï¼ŒMLR)** ï¼Œå°±æ˜¯ä½¿ç”¨æœºå™¨å­¦ä¹ çš„æŠ€æœ¯è§£å†³æ’åºé—®é¢˜ã€‚

ä¼ ç»Ÿçš„æ£€ç´¢æ¨¡å‹æ‰€è€ƒè™‘çš„å› ç´ å¹¶ä¸å¤šï¼Œä¸»è¦æ˜¯åˆ©ç”¨è¯é¢‘ã€é€†æ–‡æ¡£é¢‘ç‡å’Œæ–‡æ¡£é•¿åº¦ã€æ–‡æ¡£é‡è¦åº¦è¿™å‡ ä¸ªå› å­æ¥äººå·¥æ‹Ÿåˆæ’åºå…¬å¼ï¼Œä¸”å…¶ä¸­å¤§å¤šæ•°æ¨¡å‹éƒ½åŒ…å«å‚æ•°ï¼Œä¹Ÿå°±éœ€è¦é€šè¿‡ä¸æ–­çš„å®éªŒç¡®å®šæœ€ä½³çš„å‚æ•°ç»„åˆï¼Œä»¥æ­¤æ¥å½¢æˆç›¸å…³æ€§æ‰“åˆ†ã€‚

LTR åˆ™æ˜¯åŸºäºç‰¹å¾ï¼Œé€šè¿‡æœºå™¨å­¦ä¹ ç®—æ³•è®­ç»ƒæ¥å­¦ä¹ åˆ°æœ€ä½³çš„æ‹Ÿåˆå…¬å¼ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„æ’åºæ–¹æ³•ã€‚

æ’åºå­¦ä¹ çš„æ¨¡å‹é€šå¸¸åˆ†ä¸º**å•ç‚¹æ³•ï¼ˆPointwise Approachï¼‰**ã€**é…å¯¹æ³•ï¼ˆPairwise Approachï¼‰**å’Œ**åˆ—è¡¨æ³•ï¼ˆListwise** **Approachï¼‰**ä¸‰å¤§ç±»ï¼Œä¸‰ç§æ–¹æ³•å¹¶ä¸æ˜¯ç‰¹å®šçš„ç®—æ³•ï¼Œè€Œæ˜¯æ’åºå­¦ä¹ æ¨¡å‹çš„è®¾è®¡æ€è·¯ï¼Œä¸»è¦åŒºåˆ«ä½“ç°åœ¨æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰ã€ä»¥åŠç›¸åº”çš„æ ‡ç­¾æ ‡æ³¨æ–¹å¼å’Œä¼˜åŒ–æ–¹æ³•çš„ä¸åŒã€‚

## å•ç‚¹æ³•

å•ç‚¹æ³•æ’åºå­¦ä¹ æ¨¡å‹çš„æ¯ä¸€ä¸ªè®­ç»ƒæ ·æœ¬éƒ½ä»…ä»…æ˜¯æŸä¸€ä¸ªæŸ¥è¯¢å…³é”®å­—å’ŒæŸä¸€ä¸ªæ–‡æ¡£çš„é…å¯¹ã€‚å®ƒä»¬ä¹‹é—´æ˜¯å¦ç›¸å…³ï¼Œä¸å…¶ä»–æ–‡æ¡£å’Œå…¶ä»–æŸ¥è¯¢å…³é”®å­—éƒ½æ²¡æœ‰å…³ç³»ã€‚

## é…å¯¹æ³•

é…å¯¹æ³•çš„åŸºæœ¬æ€è·¯æ˜¯å¯¹æ ·æœ¬è¿›è¡Œä¸¤ä¸¤æ¯”è¾ƒï¼Œæ„å»ºååºæ–‡æ¡£å¯¹ï¼Œä»æ¯”è¾ƒä¸­å­¦ä¹ æ’åºï¼Œå› ä¸ºå¯¹äºä¸€ä¸ªæŸ¥è¯¢å…³é”®å­—æ¥è¯´ï¼Œæœ€é‡è¦çš„å…¶å®ä¸æ˜¯é’ˆå¯¹æŸä¸€ä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§æ˜¯å¦ä¼°è®¡å¾—å‡†ç¡®ï¼Œè€Œæ˜¯è¦èƒ½å¤Ÿæ­£ç¡®ä¼°è®¡ä¸€ç»„æ–‡æ¡£ä¹‹é—´çš„ â€œç›¸å¯¹å…³ç³»â€ã€‚

## åˆ—è¡¨æ³•

ç›¸å¯¹äºå°è¯•å­¦ä¹ æ¯ä¸€ä¸ªæ ·æœ¬æ˜¯å¦ç›¸å…³æˆ–è€…ä¸¤ä¸ªæ–‡æ¡£çš„ç›¸å¯¹æ¯”è¾ƒå…³ç³»ï¼Œåˆ—è¡¨æ³•æ’åºå­¦ä¹ çš„åŸºæœ¬æ€è·¯æ˜¯å°è¯•ç›´æ¥ä¼˜åŒ–åƒ NDCGï¼ˆNormalized Discounted Cumulative Gainï¼‰è¿™æ ·çš„æŒ‡æ ‡ï¼Œä»è€Œèƒ½å¤Ÿå­¦ä¹ åˆ°æœ€ä½³æ’åºç»“æœã€‚

# LambdaMARTç®—æ³•

LambdaMART çš„æå‡ºå…ˆåç»ç”± RankNetã€LambdaRank é€æ­¥æ¼”åŒ–è€Œæ¥ã€‚

## RankNet

RankNet çš„æ ¸å¿ƒæ˜¯**æå‡ºäº†ä¸€ç§æ¦‚ç‡æŸå¤±å‡½æ•°æ¥å­¦ä¹  Ranking Function**ï¼Œå¹¶åº”ç”¨ Ranking Function å¯¹æ–‡æ¡£è¿›è¡Œæ’åºã€‚

RankNetç½‘ç»œå°†è¾“å…¥queryçš„ç‰¹å¾å‘é‡$x\in R^n$,æ˜ å°„ä¸ºä¸€ä¸ªå®æ•°$f(x) \in R$ã€‚å…·ä½“çš„ï¼Œç»™å®šä¸¤ä¸ªqueryä¸‹çš„ä¸¤ä¸ªæ–‡æ¡£$U_i$å’Œ$U_j$ï¼Œå…¶ç‰¹å¾å€¼åˆ†åˆ«ä¸º$x_i$ï¼Œ$x_j$ï¼Œç»è¿‡RankNetè¿›è¡Œå‰å‘è®¡ç®—å¾—åˆ°ç›¸åº”çš„å¯¹åº”åˆ†æ•°$s_i = f(x_i) $,$s_j=f(x_j)$ã€‚ä½¿ç”¨$U_i \triangleright U_j$è¡¨ç¤º$U_i$æ¯”$U_j$æ’åºæ›´é å‰ï¼ˆå¦‚å¯¹æŸä¸ª query æ¥è¯´ï¼Œğ‘ˆğ‘– è¢«æ ‡è®°ä¸º â€œgoodâ€ï¼Œğ‘ˆğ‘— è¢«æ ‡è®°ä¸º â€œbadâ€ï¼‰ã€‚RankNetæŠŠæ’åºé—®é¢˜è½¬åŒ–æˆæ¯”è¾ƒä¸€ä¸ª$(U_i,U_j)$æ–‡æ¡£å¯¹çš„æ’åºé—®é¢˜ï¼Œé¦–å…ˆè®¡ç®—æ¯ä¸ªæ–‡æ¡£å¾—åˆ†ï¼Œå®šä¹‰ä½¿ç”¨ä¸‹é¢çš„å…¬å¼è®¡ç®—æ ¹æ®æ¨¡å‹å¾—åˆ†è·å–çš„$U_i$æ¯”$U_j$æ’åºæ›´é å‰çš„æ¦‚ç‡ï¼š
$$
P_{ij} \equiv P(U_i \triangleright U_j) \equiv \frac{1}{1+e^{-\sigma(s_i-s_j)}}
$$
è¿™ä¸ªæ¦‚ç‡å°±æ˜¯æ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„sigmoidå‡½æ•°ï¼Œç”±äºå‚æ•°$\sigma$åªå½±å“å‡½æ•°å½¢çŠ¶ï¼Œå¯¹æœ€ç»ˆç»“æœå½±å“ä¸å¤§ï¼Œå› æ­¤é€šå¸¸ä½¿ç”¨$\sigma=1$è¿›è¡Œç®€åŒ–ã€‚

å¯ä»¥çœ‹åˆ°RankNeté¢„æµ‹çš„åˆ†æ•°$s_i>s_j$ä¸”å·®è·è¶Šå¤§æ—¶ï¼Œ$P_{ij}$è¶Šæ¥è¿‘1ï¼Œå³$U_i$æ’åºå°±åº”è¯¥åœ¨$U_j$å‰æ¦‚ç‡è¶Šå¤§ï¼Œåä¹‹åˆ™$U_j$åœ¨$U_i$ä¹‹å‰æ¦‚ç‡è¶Šå¤§ã€‚

RankNet è¯æ˜äº†å¦‚æœçŸ¥é“ä¸€ä¸ªå¾…æ’åºæ–‡æ¡£çš„æ’åˆ—ä¸­ç›¸é‚»ä¸¤ä¸ªæ–‡æ¡£ä¹‹é—´çš„æ’åºæ¦‚ç‡ï¼Œåˆ™é€šè¿‡æ¨å¯¼å¯ä»¥ç®—å‡ºæ¯ä¸¤ä¸ªæ–‡æ¡£ä¹‹é—´çš„æ’åºæ¦‚ç‡ã€‚å› æ­¤å¯¹äºä¸€ä¸ªå¾…æ’åºæ–‡æ¡£åºåˆ—ï¼Œåªéœ€è®¡ç®—ç›¸é‚»æ–‡æ¡£ä¹‹é—´çš„æ’åºæ¦‚ç‡ï¼Œä¸éœ€è¦è®¡ç®—æ‰€æœ‰ pairï¼Œå‡å°‘è®¡ç®—é‡ã€‚

### çœŸå®çš„ç›¸å…³æ€§æ¦‚ç‡

å¯¹äºç‰¹å®šqueryï¼Œå®šä¹‰$S_{ij} \in {0,\pm 1}$ä¸ºæ–‡æ¡£$U_i$å’Œæ–‡æ¡£$U_j$è¢«æ ‡è®°çš„æ ‡ç­¾ä¹‹é—´çš„å…³è”ï¼Œå³ï¼š
$$
S_{ij} =
\begin{cases}
1,\qquad U_iæ¯”U_jæ›´ç›¸å…³ \\
0,\qquad U_iå’ŒU_jç›¸å…³æ€§ä¸€è‡´ \\
-1,\qquad U_jæ¯”U_iæ›´ç›¸å…³
\end{cases}
$$
å› æ­¤å¯ä»¥å®šä¹‰æ–‡æ¡£$U_i$æ¯”æ–‡æ¡£$U_j$æ›´ç›¸å…³çš„çœŸå®æ¦‚ç‡ä¸ºï¼š
$$
\overline{P_{ij}} = \frac{1}{1+S_{ij}}
$$

### æŸå¤±å‡½æ•°

å¯¹äºä¸€ä¸ªæ’åºï¼ŒRankNet ä»å„ä¸ª doc çš„ç›¸å¯¹å…³ç³»æ¥è¯„ä»·æ’åºç»“æœçš„å¥½åï¼Œæ’åºçš„æ•ˆæœè¶Šå¥½ï¼Œé‚£ä¹ˆæœ‰é”™è¯¯ç›¸å¯¹å…³ç³»çš„ pair å°±è¶Šå°‘ã€‚æ‰€è°“é”™è¯¯çš„ç›¸å¯¹å…³ç³»å³å¦‚æœæ ¹æ®æ¨¡å‹è¾“å‡º ğ‘ˆğ‘– æ’åœ¨ ğ‘ˆğ‘— å‰é¢ï¼Œä½†çœŸå® label ä¸º ğ‘ˆğ‘– çš„ç›¸å…³æ€§å°äº ğ‘ˆğ‘—ï¼Œé‚£ä¹ˆå°±è®°ä¸€ä¸ªé”™è¯¯ pairï¼Œ**RankNet æœ¬è´¨ä¸Šå°±æ˜¯ä»¥é”™è¯¯çš„ pair æœ€å°‘ä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œä¹Ÿå°±æ˜¯è¯´ RankNet çš„ç›®çš„æ˜¯ä¼˜åŒ–é€†åºå¯¹æ•°ã€‚è€Œåœ¨æŠ½è±¡æˆæŸå¤±å‡½æ•°æ—¶ï¼ŒRankNet å®é™…ä¸Šæ˜¯å¼•å…¥äº†æ¦‚ç‡çš„æ€æƒ³ï¼šä¸æ˜¯ç›´æ¥åˆ¤æ–­ ğ‘ˆğ‘– æ’åœ¨ ğ‘ˆğ‘— å‰é¢ï¼Œè€Œæ˜¯è¯´ ğ‘ˆğ‘– ä»¥ä¸€å®šçš„æ¦‚ç‡ P æ’åœ¨ ğ‘ˆğ‘— å‰é¢ï¼Œå³æ˜¯ä»¥é¢„æµ‹æ¦‚ç‡ä¸çœŸå®æ¦‚ç‡çš„å·®è·æœ€å°ä½œä¸ºä¼˜åŒ–ç›®æ ‡ã€‚**æœ€åï¼Œ**RankNet ä½¿ç”¨äº¤å‰ç†µï¼ˆCross Entropyï¼‰ä½œä¸ºæŸå¤±å‡½æ•°**ï¼Œæ¥è¡¡é‡ $P_{ij}$ å¯¹ $\overline{P_{ij}}$ çš„æ‹Ÿåˆç¨‹åº¦ï¼š
$$
L = - \overline{P_{ij}}logP_{ij} - (1-\overline{P_{ij}})log(1-P_{ij})
$$
åŒ–ç®€åï¼š
$$
L_{ij} = \frac{(1-S_{ij}) \sigma (s_i-s_j)}{2}+log(1+e^{-\sigma(s_i-s_j)})
$$
å½“$S_{ij}=1$æ—¶ï¼š
$$
L_{ij} = log(1+e^{-\sigma(s_i-s_j)})
$$
å½“$S_{ij}=-1$æ—¶ï¼š
$$
L_{ij} = \sigma (s_i-s_j) + log(1+e^{-\sigma(s_i-s_j)})  = log(1+e^{-\sigma(s_j-s_i)})
$$
ä»æŸå¤±å‡½æ•°å¯ä»¥çœ‹å‡ºï¼Œå¦‚æœå¯¹æ–‡æ¡£$U_i$å’Œ$U_j$çš„æ‰“åˆ†å¯ä»¥æ­£ç¡®æ‹Ÿåˆæ ‡ç­¾æ—¶ï¼Œåˆ™$L_{ij}$è¶‹äº0ï¼Œå¦åˆ™è¶‹äºçº¿æ€§å‡½æ•°ã€‚å…·ä½“çš„ï¼Œå‡å¦‚$S_{ij}$=1,åˆ™$U_i$åº”è¯¥æ¯”$U_j$æ’åºé«˜ï¼š

å¦‚æœ$s_i > s_j$åˆ™æ‹Ÿåˆçš„åˆ†æ•°å¯ä»¥æ­£ç¡®æ’åºæ–‡æ¡£ï¼š
$$
\underset{si-sj \rightarrow \infty}{lim} L_{ij}  =  \underset{si-sj \rightarrow \infty}{lim}log(1+e^{-\sigma(s_i-s_j)}) = log1=0
$$
å¦‚æœ$s_i<s_j$ï¼Œåˆ™æ‹Ÿåˆçš„åˆ†æ•°ä¸èƒ½æ­£ç¡®æ’åºæ–‡æ¡£ï¼š
$$
\underset{si-sj \rightarrow -\infty}{lim} L_{ij}  =  \underset{si-sj \rightarrow -\infty}{lim}log(1+e^{-\sigma(s_i-s_j)})=log(e^{-\sigma(s_i-s_j)}) = -\sigma(s_i-s_j)
$$
ä¸‹å›¾å±•ç¤ºäº†å½“$S_{ij}$åˆ†åˆ«å–1ï¼Œ0ï¼Œ-1æ—¶ï¼ŒæŸå¤±å‡½æ•°ä»¥$s_i-s_j$ä¸ºå˜é‡çš„å–å€¼ç¤ºæ„å›¾ï¼š

[![5WQ0JI.png](https://z3.ax1x.com/2021/10/24/5WQ0JI.png)](https://imgtu.com/i/5WQ0JI)

è¯¥æŸå¤±å‡½æ•°å…·æœ‰å¦‚ä¸‹ç‰¹ç‚¹ï¼š

1. å½“ä¸¤ä¸ªç›¸å…³æ€§ä¸åŒçš„æ–‡æ¡£ç®—å‡ºæ¥çš„æ¨¡å‹åˆ†æ•°ç›¸åŒæ—¶ï¼Œå³ ğ‘ ğ‘–=ğ‘ ğ‘—ï¼Œæ­¤æ—¶çš„æŸå¤±å‡½æ•°çš„å€¼ä¸º log2ï¼Œä¾ç„¶å¤§äº 0ï¼Œä»ä¼šå¯¹è¿™å¯¹ pair åšæƒ©ç½šï¼Œä½¿ä»–ä»¬çš„æ’åºä½ç½®åŒºåˆ†å¼€ã€‚
2. æŸå¤±å‡½æ•°æ˜¯ä¸€ä¸ªç±»çº¿æ€§å‡½æ•°ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘å¼‚å¸¸æ ·æœ¬æ•°æ®å¯¹æ¨¡å‹çš„å½±å“ï¼Œå› æ­¤å…·æœ‰é²æ£’æ€§ã€‚

Ranknet æœ€ç»ˆç›®æ ‡æ˜¯è®­ç»ƒå‡ºä¸€ä¸ªæ‰“åˆ†å‡½æ•°$s=f(x,w)$ï¼Œä½¿å¾—æ‰€æœ‰ pair çš„æ’åºæ¦‚ç‡ä¼°è®¡çš„æŸå¤±æœ€å°ï¼Œå³ï¼š
$$
L = \sum_{(i,j) \in I} L_{ij}
$$
å…¶ä¸­ï¼Œğ¼ è¡¨ç¤ºæ‰€æœ‰åœ¨åŒä¸€ query ä¸‹ï¼Œä¸”å…·æœ‰ä¸åŒç›¸å…³æ€§åˆ¤æ–­çš„ doc pairï¼Œæ¯ä¸ª pair æœ‰ä¸”ä»…æœ‰ä¸€æ¬¡ã€‚

é€šå¸¸è¯¥æ‰“åˆ†å‡½æ•°åªè¦å…‰æ»‘å¯å¯¼å³å¯ã€‚

### å‚æ•°æ›´æ–°

RankNet é‡‡ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹ä¼˜åŒ–æŸå¤±å‡½æ•°ï¼Œä¹Ÿå°±æ˜¯åå‘ä¼ æ’­è¿‡ç¨‹ï¼Œé‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£å¹¶æ›´æ–°å‚æ•°ï¼š
$$
w_k = w_k - \eta \frac{\partial L} {\partial w_k}
$$
å…¶ä¸­$\eta$ä¸ºå­¦ä¹ ç‡ã€‚RankNet æ˜¯ä¸€ç§æ–¹æ³•æ¡†æ¶ï¼Œå› æ­¤è¿™é‡Œçš„ ğ‘¤ğ‘˜ å¯ä»¥æ˜¯ NNã€LRã€GBDT ç­‰ç®—æ³•çš„æƒé‡ã€‚

å¯¹RankNetçš„æ¢¯åº¦$\frac{\partial L} {\partial w_k}$è¿›è¡Œå› å¼åˆ†è§£ï¼Œæœ‰ï¼š
$$
\frac{\partial L} {\partial w_k} = \sum_{(i,j)\in I}\frac{\partial L_{ij}} {\partial w_k}=\sum_{(i,j)\in I}[\frac{\partial L_{ij}} {\partial s_i} \frac{\partial s_i}{\partial w_k} + \frac{\partial L_{ij}} {\partial s_j} \frac{\partial s_j}{\partial w_k}]
$$
å…¶ä¸­ï¼š
$$
\frac{\partial L_{ij}} {\partial s_i} = \\ \frac{\partial \{ \frac{(1-S_{ij}) \sigma (s_i-s_j)}{2}+log(1+e^{-\sigma(s_i-s_j)})\}}{\partial s_i} = \\ \sigma[\frac{(1-S_{ij})}{2}-\frac 1 {1+e^{\sigma(s_i-s_j)}}] =\\ -\frac{\partial L_{ij}} {\partial s_j}
$$
$s_i$å’Œ$s_j$å¯¹$w_k$çš„åå¯¼æ•°å¯ä»¥æ ¹æ®å®é™…ä½¿ç”¨çš„æ¨¡å‹æ±‚çš„ã€‚

ç»“æœæ’åºæœ€ç»ˆç”±æ¨¡å‹å¾—åˆ†$s_i$ç¡®å®šï¼Œå…¶æ¢¯åº¦å¾ˆå…³é”®ï¼Œå®šä¹‰ï¼š
$$
\lambda_{ij} \overset{def}{=} \frac{\partial L_{ij}} {\partial s_i} = -\frac{\partial L_{ij}} {\partial s_j} =\\ \sigma[\frac{(1-S_{ij})}{2}-\frac 1 {1+e^{\sigma(s_i-s_j)}}]
$$
å¯¹äºæœ‰åºå¯¹$(i,j)$,æœ‰$S_{ij}= 1$ï¼Œäºæ˜¯åŒ–ç®€ï¼š
$$
\lambda_{ij} \overset{def}=-\frac 1 {1+e^{\sigma(s_i-s_j)}}
$$
æ­¤æ—¶$\lambda_{ij}$å§‹ç»ˆä¸ºè´Ÿæ•°ï¼Œä½¿ç”¨æ¢¯åº¦æ›´æ–°æ—¶ï¼Œå°†ä¼šä½¿$s_i$å¢å¤§ï¼Œå› æ­¤ï¼Œå¯¹äºæ–‡æ¡£$U_i$æ¥è¯´ï¼ŒçœŸå®çš„ç›¸å…³æ€§åœ¨å…¶åçš„æ–‡æ¡£å°†ä¿ƒè¿›å…¶åˆ†æ•°å¢åŠ ã€‚å³ç»™å…¶ä¸€ä¸ªå‘ä¸Šæ¨çš„åŠ›ã€‚

ç”±äºï¼š
$$
\frac{\partial L_{ij}} {\partial s_i}  = -\frac{\partial L_{ij}} {\partial s_j}
$$
å¯¹äº$s_j$æ¥è¯´ï¼Œå…¶å¯¼æ•°å‡ä¸ºæ­£æ•°ï¼Œåˆ™åœ¨æ›´æ–°æ¨¡å‹æ—¶ä½¿$s_j$å˜å°ï¼Œå› æ­¤å¯¹åº”ä»»æ„æ–‡æ¡£æ¥è¯´ï¼ŒçœŸå®çš„ç›¸å…³æ€§åœ¨å…¶å‰çš„æ–‡æ¡£éƒ½å°†ä¿ƒä½¿å…¶åˆ†æ•°å‡å°ï¼Œå³ç»™å…¶ä¸€ä¸ªå‘ä¸‹æ¨çš„åŠ›ã€‚

è¿™ä¸ª$lambda_{ij}$å³ä¸ºæ¥ä¸‹æ¥ä»‹ç»çš„LambdaRank å’Œ LambdaMART ä¸­çš„ Lambdaï¼Œç§°ä¸º Lambda æ¢¯åº¦ã€‚

## LambdaRank

RankNet æœ¬è´¨ä¸Šå°±æ˜¯ä»¥é”™è¯¯çš„ pair æœ€å°‘ä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œä¹Ÿå°±æ˜¯è¯´ RankNet çš„ç›´æ¥ç›®çš„å°±æ˜¯ä¼˜åŒ–é€†åºå¯¹æ•°ï¼ˆpairwise errorï¼‰ï¼Œè¿™ç§æ–¹å¼ä¸€å®šç¨‹åº¦ä¸Šèƒ½å¤Ÿè§£å†³ä¸€äº›æ’åºé—®é¢˜ï¼Œä½†å®ƒå¹¶ä¸å®Œç¾ã€‚

é€†åºå¯¹æ•°ï¼ˆpairwise errorï¼‰è¡¨ç¤ºä¸€ä¸ªæ’åˆ—ä¸­ï¼ŒæŠ½æŸ¥ä»»æ„ä¸¤ä¸ª itemï¼Œä¸€å…±æœ‰ $C_2^n$ ç§å¯èƒ½çš„ç»„åˆï¼Œå¦‚æœè¿™ä¸¤ä¸ª item çš„ä¹‹é—´çš„ç›¸å¯¹æ’åºé”™è¯¯ï¼Œé€†åºå¯¹æ•°é‡å¢åŠ  1ã€‚

**é€†åºå¯¹æ•°çš„å®è´¨å°±æ˜¯æ’å…¥æ’åºè¿‡ç¨‹ä¸­è¦ç§»åŠ¨å…ƒç´ çš„æ¬¡æ•°ã€‚ç›´è§‚ç†è§£ä¸ºè¦æƒ³æŠŠæŸä¸ªå…ƒç´ ç§»åŠ¨åˆ°æœ€ä¼˜æ’åºä½ç½®ï¼Œéœ€è¦ç§»åŠ¨å¤šå°‘æ¬¡ï¼Œä¸¤ä¸ªå…ƒç´ å°±æ˜¯äºŒè€…ç§»åŠ¨æ¬¡æ•°çš„å’Œã€‚**

ä¾‹å¦‚ï¼Œå¯¹æŸä¸ª Queryï¼Œå’Œå®ƒç›¸å…³çš„æ–‡ç« æœ‰ä¸¤ä¸ªï¼Œè®°ä¸º (ğ‘„,[ğ·1,ğ·2]) ã€‚

1. å¦‚æœæ¨¡å‹ ğ‘“(â‹…) å¯¹æ­¤ Query è¿”å›çš„ n æ¡ç»“æœä¸­ï¼Œ ğ·1,ğ·2 åˆ†åˆ«ä½äºå‰ä¸¤ä½ï¼Œé‚£ä¹ˆ pairwise error å°±ä¸º 0ï¼›
2. å¦‚æœæ¨¡å‹ ğ‘“(â‹…) å¯¹æ­¤ Query è¿”å›çš„ n æ¡ç»“æœä¸­ï¼Œ ğ·1,ğ·2 åˆ†åˆ«ä½äºç¬¬ 1 ä½å’Œç¬¬ n ä½ï¼Œé‚£ä¹ˆ pairwise error ä¸º n-2ï¼›
3. å¦‚æœæ¨¡å‹ ğ‘“(â‹…) å¯¹æ­¤ Query è¿”å›çš„ n æ¡ç»“æœä¸­ï¼Œ ğ·1,ğ·2 åˆ†åˆ«ä½äºç¬¬ 2 å’Œç¬¬ 3 ä½ï¼Œé‚£ä¹ˆ pair-wise error ä¸º 2ï¼›

å‡è®¾RankNetç»è¿‡ä¸¤è½®è¿­ä»£å®ç°ä¸‹å›¾æ‰€ç¤ºçš„é¡ºåºä¼˜åŒ–ï¼š

[![5WNCo8.png](https://z3.ax1x.com/2021/10/24/5WNCo8.png)](https://imgtu.com/i/5WNCo8)

ç¬¬ä¸€è½®çš„æ—¶å€™é€†åºå¯¹æ•°ä¸º 13ï¼Œç¬¬äºŒè½®ä¸º 3 + 8 = 11ï¼Œé€†åºå¯¹æ•°ä» 13 ä¼˜åŒ–åˆ° 11ï¼ŒæŸå¤±ç¡®å®æ˜¯å‡å°äº†ï¼Œå¦‚æœç”¨ AUC ä½œä¸ºè¯„ä»·æŒ‡æ ‡ï¼Œä¹Ÿå¯ä»¥è·å¾—æŒ‡æ ‡çš„æå‡ã€‚ä½†å®é™…ä¸Šï¼Œæˆ‘ä»¬ä¸éš¾å‘ç°ï¼Œä¼˜åŒ–é€†åºå¯¹æ•°å¹¶æ²¡æœ‰è€ƒè™‘ä½ç½®çš„æƒé‡ï¼Œè¿™ä¸æˆ‘ä»¬å®é™…å¸Œæœ›çš„æ’åºç›®æ ‡ä¸ä¸€è‡´ã€‚ä¸‹ä¸€è½®è¿­ä»£ï¼ŒRankNet ä¸ºäº†è·å¾—æ›´å¤§çš„é€†åºå¯¹æ•°çš„å‡å°ï¼Œä¼šæŒ‰ç…§é»‘è‰²ç®­å¤´é‚£æ ·çš„è¶‹åŠ¿ï¼Œæ’åé å‰çš„æ–‡æ¡£ä¼˜åŒ–åŠ›åº¦ä¼šå‡å¼±ï¼Œæ›´å¤šçš„é‡å¿ƒæ˜¯æŠŠåä¸€ä¸ªæ–‡æ¡£å¾€å‰æ’ï¼Œè¿™ä¸æˆ‘ä»¬æœç´¢æ’åºç›®æ ‡æ˜¯ä¸ä¸€è‡´çš„ï¼Œæˆ‘ä»¬æ›´å¸Œæœ›å‡ºç°çº¢è‰²ç®­å¤´çš„è¶‹åŠ¿ï¼Œä¼˜åŒ–çš„é‡ç‚¹æ”¾åœ¨æ’åé å‰çš„æ–‡æ¡£ï¼Œå°½å¯èƒ½åœ°å…ˆè®©å®ƒæ’åœ¨æœ€ä¼˜ä½ç½®ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€ä¸ªèƒ½å¤Ÿè€ƒè™‘ä½ç½®æƒé‡çš„ä¼˜åŒ–æŒ‡æ ‡ã€‚

**RankNet ä»¥ä¼˜åŒ–é€†åºå¯¹æ•°ä¸ºç›®æ ‡ï¼Œå¹¶æ²¡æœ‰è€ƒè™‘ä½ç½®çš„æƒé‡ï¼Œè¿™ç§ä¼˜åŒ–æ–¹å¼å¯¹ AUC è¿™ç±»è¯„ä»·æŒ‡æ ‡æ¯”è¾ƒå‹å¥½ï¼Œä½†å®é™…çš„æ’åºç»“æœä¸ç°å®çš„æ’åºéœ€æ±‚ä¸ä¸€è‡´ï¼Œç°å®ä¸­çš„æ’åºéœ€æ±‚æ›´åŠ æ³¨é‡å¤´éƒ¨çš„ç›¸å…³åº¦ï¼Œæ’åºè¯„ä»·æŒ‡æ ‡é€‰ç”¨ NDCG è¿™ä¸€ç±»çš„æŒ‡æ ‡æ‰æ›´åŠ ç¬¦åˆå®é™…éœ€æ±‚ã€‚è€Œ RankNet è¿™ç§ä»¥ä¼˜åŒ–é€†åºå¯¹æ•°ä¸ºç›®çš„çš„äº¤å‰ç†µæŸå¤±ï¼Œå¹¶ä¸èƒ½ç›´æ¥æˆ–è€…é—´æ¥ä¼˜åŒ– NDCG è¿™æ ·çš„æŒ‡æ ‡ã€‚**

**å¯¹äºç»å¤§å¤šæ•°çš„ä¼˜åŒ–è¿‡ç¨‹æ¥è¯´ï¼Œç›®æ ‡å‡½æ•°å¾ˆå¤šæ—¶å€™ä»…ä»…æ˜¯ä¸ºäº†æ¨å¯¼æ¢¯åº¦è€Œå­˜åœ¨çš„ã€‚è€Œå¦‚æœæˆ‘ä»¬ç›´æ¥å°±å¾—åˆ°äº†æ¢¯åº¦ï¼Œé‚£è‡ªç„¶å°±ä¸éœ€è¦ç›®æ ‡å‡½æ•°äº†ã€‚**

å¾®è½¯å­¦è€…ç»è¿‡åˆ†æï¼Œå°±ç›´æ¥æŠŠ RankNet æœ€åå¾—åˆ°çš„ Lambda æ¢¯åº¦æ‹¿æ¥ä½œä¸º LambdaRank çš„æ¢¯åº¦æ¥ç”¨äº†ï¼Œè¿™ä¹Ÿæ˜¯ LambdaRank ä¸­ Lambda çš„å«ä¹‰ã€‚è¿™æ ·æˆ‘ä»¬ä¾¿çŸ¥é“äº† LambdaRank å…¶å®æ˜¯ä¸€ä¸ªç»éªŒç®—æ³•ï¼Œå®ƒä¸æ˜¯é€šè¿‡æ˜¾ç¤ºå®šä¹‰æŸå¤±å‡½æ•°å†æ±‚æ¢¯åº¦çš„æ–¹å¼å¯¹æ’åºé—®é¢˜è¿›è¡Œæ±‚è§£ï¼Œè€Œæ˜¯åˆ†ææ’åºé—®é¢˜éœ€è¦çš„æ¢¯åº¦çš„ç‰©ç†æ„ä¹‰ï¼Œç›´æ¥å®šä¹‰æ¢¯åº¦ï¼Œå³ Lambda æ¢¯åº¦ã€‚æœ‰äº†æ¢¯åº¦ï¼Œå°±ä¸ç”¨å…³å¿ƒæŸå¤±å‡½æ•°æ˜¯å¦è¿ç»­ã€æ˜¯å¦å¯å¾®äº†ï¼Œæ‰€ä»¥ï¼Œå¾®è½¯å­¦è€…ç›´æ¥æŠŠ NDCG è¿™ä¸ªæ›´å®Œå–„çš„è¯„ä»·æŒ‡æ ‡ä¸ Lambda æ¢¯åº¦ç»“åˆäº†èµ·æ¥ï¼Œå°±å½¢æˆäº† LambdaRankã€‚

é¦–å…ˆæ¥åˆ†æä¸€ä¸‹lambdaæ¢¯åº¦çš„æ„ä¹‰ï¼šLambdaRank ä¸­çš„ Lambd å…¶å®å°±æ˜¯ RankNet ä¸­çš„æ¢¯åº¦ $\lambda_{ij}$ï¼Œ $\lambda_{ij}$å¯ä»¥çœ‹æˆæ˜¯ ğ‘ˆğ‘– å’Œ ğ‘ˆğ‘— ä¸­é—´çš„ä½œç”¨åŠ›ï¼Œä»£è¡¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¼˜åŒ–çš„æ–¹å‘å’Œå¼ºåº¦ã€‚å¦‚æœ ğ‘ˆğ‘–âŠ³ğ‘ˆğ‘—ï¼Œåˆ™ ğ‘ˆğ‘— ä¼šç»™äºˆ ğ‘ˆğ‘– å‘ä¸Šçš„å¤§å°ä¸º  $\lambda_{ij}$ çš„æ¨åŠ¨åŠ›ï¼Œè€Œå¯¹åº”åœ° ğ‘ˆğ‘– ä¼šç»™äºˆ ğ‘ˆğ‘— å‘ä¸‹çš„å¤§å°ä¸º  $\lambda_{ij}$ çš„æ¨åŠ¨åŠ›ã€‚å¯¹åº”ä¸Šé¢çš„é‚£å¼ å›¾ï¼ŒLambda çš„ç‰©ç†æ„ä¹‰å¯ä»¥ç†è§£ä¸ºå›¾ä¸­ç®­å¤´ï¼Œå³ä¼˜åŒ–è¶‹åŠ¿ã€‚

å› æ­¤ï¼Œç›´æ¥ä½¿ç”¨$\vert \triangle_{NDCG} \vert $ä¹˜ä»¥$\lambda_{ij}$ï¼Œå°±å¯ä»¥å¾—åˆ°LambdaRankçš„Lambdaï¼Œå³ï¼š
$$
\lambda_{i,j} \overset{def}=\frac{\partial L{(s_i-s_j)}}{\partial s_i} =-\frac 1 {1+e^{\sigma(s_i-s_j)}} \vert \triangle_{NDCG} \vert
$$
å…¶ä¸­$\vert \triangle_{NDCG} \vert $ä¸º$U_i$å’Œ$U_j$äº¤æ¢æ’åºä½ç½®å¾—åˆ°çš„NDCGå·®å€¼çš„**ç»å¯¹å€¼**ã€‚é€šè¿‡å‰æ–‡çš„æè¿°æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼ŒNDCGæŒ‡æ ‡è€ƒè™‘äº†æ’åºä½ç½®çš„å½±å“ï¼Œ$\vert \triangle_{NDCG} \vert $å€¼ä¸ºåœ¨å…¨éƒ¨å¬å›é˜Ÿåˆ—ä¸­ä»…è€ƒè™‘iå’Œjä¸¤ä¸ªæ–‡æ¡£æ—¶ï¼Œäº’æ¢ä½ç½®å¯¹æœ€ç»ˆDNCGåˆ†æ•°çš„å½±å“ã€‚

å¯¹äº$U_i \rhd U_j$æ¥è¯´ï¼Œå¦‚æœè¿”å›çš„åˆ†æ•°æ’åºå$U_i$å¯¹åº”$index_1$,$U_j$å¯¹åº”$index_j$ã€‚åˆ™ï¼š
$$
 \triangle_{NDCG}   = \frac 1 {IDCG} [ (\sum_{k \in I, k \notin {i,j} } \frac{rel_k}{log_2(index_k+1)})+ \frac{rel_i}{log_2{(index_j+1)}}+\frac{rel_j}{log_2{(index_i+1)}} - \\ (\sum_{k \in I, k \notin {i,j} } \frac{rel_k}{log_2(index_k+1)})+ \frac{rel_i}{log_2{(index_j+1)}} - \frac{rel_i}{log_2{(index_i+1)}}-\frac{rel_j}{log_2{(index_j+1)}}]=\\
\frac 1 {IDCG} (rel_i-rel_j)(\frac 1 {log(index_j+1)}-\frac 1 {log(index_i+1)})
$$
ä»ä¸Šè¯•æˆ‘ä»¬å¯ä»¥å¾—çŸ¥ï¼Œå¤§ä¸¤ä¸ªæ–‡æ¡£çš„çœŸå®åˆ†æ•°å·®è¶Šå¤§æ—¶ï¼Œç»“æœçš„$\vert \triangle_{NDCG} \vert $å€¼è¶Šå¤§ï¼Œè¿™æ ·å°±èµ·åˆ°äº†å¯¹äºæ’åé«˜å¹¶ä¸”ç›¸å…³æ€§é«˜çš„æ–‡æ¡£æ›´å¿«åœ°å‘ä¸Šæ¨åŠ¨ï¼Œè€Œæ’åä½è€Œä¸”ç›¸å…³æ€§è¾ƒä½çš„æ–‡æ¡£è¾ƒæ…¢åœ°å‘ä¸Šæ¨åŠ¨ã€‚

å¯¹äºç‰¹å®š$U_i$ï¼Œç´¯åŠ å…¶ä»–æ‰€æœ‰æ’åºé¡¹çš„å½±å“ï¼Œå¾—åˆ°ï¼š
$$
\lambda_i = \sum_{(i,j)\in I,i \rhd j}\lambda_{ij} + \sum_{(k,i)\in I,k \rhd i}\lambda_{ki}=\sum_{(i,j)\in I,i \rhd j}\lambda_{ij} - \sum_{(k,i)\in I,k \rhd i}\lambda_{ik}
$$
å³ï¼Œå¯¹äºæ¯ä¸ªæ–‡æ¡£ï¼Œåˆ†åˆ«è®¡ç®—ä¸æ’åœ¨å…¶åçš„æ‰€æœ‰æ–‡æ¡£çš„$\lambda_{ij}$å’Œä¸æ’åœ¨å…¶å‰çš„æ‰€æœ‰æ–‡æ¡£çš„$\lambda_{ki}$,å¯¹è¿™äº›å€¼è¿›è¡Œæ±‚å’Œã€‚

ä»ä¹‹å‰çš„åˆ†ææˆ‘ä»¬çŸ¥é“ï¼Œæ’åœ¨å…¶åçš„æ–‡æ¡£ç»™äºˆå½“å‰æ–‡æ¡£ä¸€ä¸ªå‘ä¸Šæ¨åŠ¨çš„åŠ›ï¼Œæ’åœ¨å…¶åçš„æ–‡æ¡£ç»™äºˆå…¶å‘ä¸‹æ¨åŠ¨çš„åŠ›ã€‚å› æ­¤ä¸ºäº†ä¿è¯ç›¸å…³æ€§æ›´é«˜çš„æ–‡æ¡£æ’åºæ›´é«˜(å¯¹ç›¸å…³æ€§ä½çš„ç±»ä¼¼ï¼Œè®©å…¶æ’åºæ›´é å)ï¼Œå› æ­¤æˆ‘ä»¬ç›´æ¥æ‹Ÿåˆ$\lambda_i$å³å¯ã€‚

å¦‚æœå°†$\lambda_i$çœ‹åšä¸€ä¸ªå¯¼æ•°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æ¨å‡ºå…¶æ•ˆç”¨å‡½æ•°ä¸ºï¼š
$$
C_{ij} = log(1+e^{-\sigma(si-sj)})\vert \triangle_{NDCG} \vert
$$

## LambdaMARTç®—æ³•

LambdaRank é‡æ–°å®šä¹‰äº†æ¢¯åº¦ï¼Œèµ‹äºˆäº†æ¢¯åº¦æ–°çš„ç‰©ç†æ„ä¹‰ï¼Œå› æ­¤ï¼Œæ‰€æœ‰å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£çš„æ¨¡å‹éƒ½å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ¢¯åº¦ï¼ŒåŸºäºå†³ç­–æ ‘çš„ MART å°±æ˜¯å…¶ä¸­ä¸€ç§ï¼Œå°†æ¢¯åº¦ Lambda å’Œ MART ç»“åˆå°±æ˜¯å¤§åé¼é¼çš„ LambdaMARTã€‚

MARTå³ä¸ºGBDTï¼ŒLambdaMART åªæ˜¯åœ¨ GBDT çš„è¿‡ç¨‹ä¸­åšäº†ä¸€ä¸ªå¾ˆå°çš„ä¿®æ”¹ã€‚åŸå§‹ GBDT çš„åŸç†æ˜¯ç›´æ¥åœ¨å‡½æ•°ç©ºé—´å¯¹å‡½æ•°è¿›è¡Œæ±‚è§£æ¨¡å‹ï¼Œç»“æœç”±è®¸å¤šæ£µæ ‘ç»„æˆï¼Œæ¯æ£µæ ‘çš„æ‹Ÿåˆç›®æ ‡æ˜¯æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œè€Œåœ¨ LambdaMART ä¸­è¿™ä¸ªæ¢¯åº¦å°±æ¢æˆäº† Lambda æ¢¯åº¦ï¼Œè¿™æ ·å°±ä½¿å¾— GBDT å¹¶ä¸æ˜¯ç›´æ¥ä¼˜åŒ–äºŒåˆ†åˆ†ç±»é—®é¢˜ï¼Œè€Œæ˜¯ä¸€ä¸ªæ”¹è£…äº†çš„äºŒåˆ†åˆ†ç±»é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯åœ¨ä¼˜åŒ–çš„æ—¶å€™ä¼˜å…ˆè€ƒè™‘èƒ½å¤Ÿè¿›ä¸€æ­¥æ”¹è¿› NDCG çš„æ–¹å‘ã€‚

LambdaMARTæ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š

[![5fujCq.png](https://z3.ax1x.com/2021/10/24/5fujCq.png)](https://imgtu.com/i/5fujCq)

å…¶æ‰§è¡Œé€»è¾‘å¦‚ä¸‹ï¼š

0. éšå«æ­¥éª¤ï¼šä½¿ç”¨å½“å‰æ¨¡å‹$F(x)$çš„è¾“å‡º$s$è®¡ç®—æ•ˆç”¨å‡½æ•°$C$ã€‚è¿™é‡Œå®é™…å¹¶ä¸ä¼šæ‰§è¡Œï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»ç›´æ¥æ‰¾åˆ°äº†å…¶æ¢¯åº¦ï¼Œä¸éœ€è¦é€šè¿‡æ•ˆç”¨å‡½æ•°æ¥è®¡ç®—æ¢¯åº¦ã€‚è¿™é‡Œæ˜¯æé†’è¯»è€…ï¼Œæ¯è½®å¾ªç¯ä¼˜åŒ–çš„ç›®æ ‡ï¼Œä¾ç„¶æ˜¯æœ€å°åŒ–æ•ˆç”¨å‡½æ•°ã€‚

1. æ¯æ£µæ ‘çš„è®­ç»ƒéƒ½ä¼šå…ˆéå†æ‰€æœ‰çš„æ•°æ®ï¼Œè®¡ç®—æ¯ä¸ªpairï¼ˆåŒä¸€ä¸ªqueryä¸‹çš„ï¼‰äº’æ¢ä½ç½®å¯¼è‡´çš„æŒ‡æ ‡å˜åŒ–$\vert \triangle_{NDCG} \vert $ä»¥åŠlambdaå€¼ï¼Œå³ï¼š
   $$
   \lambda_{i,j} =-\frac 1 {1+e^{\sigma(s_i-s_j)}} \vert \triangle_{NDCG} \vert
   $$
   ä¹‹åè®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„Lambdaï¼š
   $$
   \lambda_i =\sum_{(i,j)\in I,i \rhd j}\lambda_{ij} - \sum_{(k,i)\in I,k \rhd i}\lambda_{ik}
   $$
   é€šè¿‡lambdaæ¢¯åº¦æˆ‘ä»¬æ¨å‡ºæ•ˆç”¨å‡½æ•°ä¸ºï¼š
   $$
   C(s_i-sj) = \sum_{i,j\in I}{log(1+e^{-\sigma(si-sj)})\vert \triangle_{NDCG} \vert}
   $$
   ç”±æ­¤å¯å¾—ï¼š
   $$
   \frac{\partial C} {\partial s_i} = \sum_{i,j \in I} \frac{-\sigma\vert \triangle_{NDCG} \vert }{1+e^{\sigma (s_i -s_j)}}
   $$
   å®šä¹‰ï¼š
   $$
   \rho_{ij} = \frac 1 {1+e^{\sigma (s_i -s_j)}}=\frac{-\lambda_{ij}}{\sigma\vert \triangle_{NDCG} \vert }
   $$
   åˆ™ï¼š
   $$
   y_i = \lambda_i =\frac{\partial C} {\partial s_i} = \sum_{i,j \in I} {-\sigma}\vert \triangle_{NDCG} \vert \rho_{ij}
   $$
   è®¡ç®—æ¯ä¸ª$ y_i$å¯¹$\lambda_i$çš„å€’æ•°ï¼Œç”¨äºåç»­ä½¿ç”¨ç‰›é¡¿æ³•æ±‚è§£å¶å­èŠ‚ç‚¹çš„æ•°å€¼ï¼š
   $$
   w_i =\frac{\partial y_i} {\partial s_i} = \frac{\partial^2 C} {\partial s_i^2} = \sum_{i,j\in I}\sigma^2 \vert \triangle_{NDCG} \vert \rho_{ij}(1-\rho_{ij}) = \sum_{i,j \in I} \sigma^2 (-\lambda_{ij}) (1+\frac{\lambda_{ij}}{\sigma \vert \triangle_{NDCG} \vert})
   $$

2. ä½¿ç”¨$(X, \lambda)$,æ‹Ÿåˆä¸€ä¸ªå†³ç­–å›å½’æ ‘ï¼Œä½œä¸ºæœ¬è½®å¾ªç¯ç”Ÿæˆçš„å¼±åˆ†ç±»å™¨ã€‚<font color=red>æ³¨æ„ï¼Œè¿™é‡Œçš„$\lambda$æ˜¯ä¸Šä¸€æ­¥çš„$-y_i$ï¼Œå³è´Ÿæ¢¯åº¦</font>ã€‚

3. å¯¹ç¬¬äºŒæ­¥ç”Ÿæˆçš„å›å½’æ ‘ï¼Œè®¡ç®—æ¯ä¸ªå¶å­èŠ‚ç‚¹çš„æ•°å€¼ï¼Œé‡‡ç”¨ç‰›é¡¿è¿­ä»£æ³•æ±‚è§£ã€‚

   ç‰›é¡¿æ³•æ˜¯å¯¹äºä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨æ³°å‹’å±•å¼€ï¼Œå¿½ç•¥é«˜é˜¶é¡¹çš„ä¸€ä¸ªè¿­ä»£ä¼˜åŒ–æ–¹å¼ã€‚ä¾‹å¦‚å¯¹äºä»»æ„$f(X)$,åœ¨ä»»æ„ç‚¹$x_n$è¿›è¡Œæ³°å‹’å±•ç¤ºä¸ºï¼š
   $$
   f(x) = f(x_n) + \frac{f^{(1)}(x_n)}{1}(x-x_n) + \frac{f^{(2)}(x_n)}{2}(x-x_n)^2 + \sum_{k=3}^{\infty}\frac{f^{(k)}(x_n)}{k}(x-x_n)^k \approx \\ f(x_n) + \frac{f^{(1)}(x_n)}{1}(x-x_n) + \frac{f^{(2)}(x_n)}{2}(x-x_n)^2
   $$
   ç‰›é¡¿æ³•å¿½ç•¥é«˜é˜¶é¡¹ï¼Œä¹‹åå¦‚æœè¦ä½¿å¾—å‡½æ•°$f(x)$æœ€å°ï¼Œåˆ™æ‰¾åˆ°$f(x)$æ¢¯åº¦ä¸º0ï¼Œå³ï¼š
   $$
   f^{(1)}(x_n) + f^{(2)}(x_n)(x-x_n) = 0 => \\
   x = x_n - \frac{f^{(1)}(x_n)}{f^{(2)}(x_n)}
   $$
   GBDTå¯¹çš„æ¯ä¸ªå¶èŠ‚ç‚¹çš„è®¡ç®—ä¸ºï¼š 
   $$
   c_{lk} = arg \ \underset{c}{min} \sum_{x_i \in R_{lk}}L(y_i, f_{t-1}(x_i)+c)
   $$
   

   å¯¹åº”åˆ°lambdaMARTç®—æ³•ï¼Œå½“å‰çš„$x_i$å¯¹åº”ä¸ºè¾“å…¥$x_i$ã€‚å‡½æ•°$f_{t-1}(x)$å¯¹åº”äºå½“å‰è¾“å‡ºçš„$s_i$åˆ†æ•°ã€‚$L$å¯¹åº”äºæ•ˆç”¨å‡½æ•°$C$ã€‚è¿™é‡Œ$s_i$çš„å®é™…æ˜¯å½“å‰å·²ç»è®­ç»ƒçš„å¼±æ¨¡å‹çš„åŠ å’Œå³:$s_i=F_{k-1}(x_i)$ï¼Œå…¶ä¸­$k$-1ä¸ºå½“å‰è®­ç»ƒçš„å›å½’æ ‘æ•°é‡ã€‚

   åˆ©ç”¨ç‰›é¡¿æ³•ï¼Œè·å–æœ€å°åŒ–æ•ˆç”¨å‡½æ•°æ—¶è¾“å…¥ä¸ºï¼š
   $$
   f_{t-1}(x_i)+c = -\frac{\lambda_i}{w_i}
   $$
   æ­¤æ—¶$s_i-s_j = f_{t-1}(x_i)$ï¼Œå› æ­¤ï¼š
   $$
   c_{lk} = -\frac{\lambda_i}{w_i}
   $$
   å¯¹åº”ä¸Šå›¾$\gamma_{lk}$çš„è®¡ç®—ï¼š
   $$
   \gamma_{lk} = \frac{\sum_{x_i in R_{lK}} y_i}{\sum_{x_i \in R_{lk}}w_i} = \frac{\sum_{x_i in R_{lK}} \frac{\partial C} {\partial s_i}}{\sum_{x_i \in R_{lk}}\frac{\partial^{(2)} C} {\partial s_i^2}} = \frac{\sum_{x_i in R_{lK}} \lambda_i}{\sum_{x_i \in R_{lk}}w_i}
   $$
   è¿™é‡Œç¼ºå¤±ä¸€ä¸ªè´Ÿå·ï¼Œæˆ‘ç†è§£æ˜¯ä¸Šå›¾æœ‰é”™è¯¯ï¼Œåœ¨ä½¿ç”¨å›å½’æ ‘æ‹Ÿåˆçš„æ—¶å€™ï¼Œæ‹Ÿåˆçš„æ˜¯è´Ÿæ¢¯åº¦ï¼Œå³$y_i = -\lambda_i$ï¼Œå¯¹äºäºŒé˜¶å¯¼ï¼Œåº”è¯¥ä½¿ç”¨åŸ$\lambda$çš„äºŒé˜¶å¯¼ï¼Œè¿™æ ·ç›¸é™¤å°±å­˜åœ¨ä¸€ä¸ªè´Ÿå·ï¼Œè¿™æ ·æ±‚å‡ºçš„$\gamma_{lk}$æ‰æ˜¯ä¸Šå›¾ä¸­çš„å½¢å¼ã€‚

4. æ¨¡å‹æ›´æ–°

   ä½¿ç”¨ç‰›é¡¿æ³•æ›´æ–°æ¨¡å‹ï¼Œå°†ç¬¬ké¢—æ ‘å¢åŠ åˆ°æ¨¡å‹ä¸­ã€‚æ¨¡å‹æ›´æ–°å¦‚ä¸‹ï¼š
   $$
   F_k(x_i) = F_{k-1} + \eta \sum_l \gamma_{lk}I(x_i \in R_{lk})
   $$
   å…¶ä¸­$\eta$ä¸ºå­¦ä¹ ç‡ã€‚

 



# ç¨‹åºæ‰§è¡Œé€»è¾‘

```python
def train(train_data, valid_data, model_save_path):
    with open(train_data) as trainfile, \
            open(valid_data) as valifile:
        TX, Ty, sample_weight, Tqids, _ = pyltr.data.letor.read_dataset(trainfile, one_indexed=True, has_weight=True)
        VX, Vy, _, Vqids, _ = pyltr.data.letor.read_dataset(valifile, one_indexed=True, has_weight=False)
    # old_k is 4 
    metric_train = pyltr.metrics.NDCG(k=20)
    metric_valid = pyltr_note.metrics.NDCG(k=4)
    monitor = pyltr.models.monitors.ValidationMonitor(
            VX, Vy, Vqids, metric=metric_valid, stop_after=250)
    
    print "load data finished ....."
    print 'begin to train ......'


#pjz
    model = pyltr.models.LambdaMART(
        metric=metric_train,
        n_estimators=1000,
        learning_rate=0.01,
        max_features=0.5,
        query_subsample=0.75,
        max_leaf_nodes=20,
        min_samples_leaf=64,
        verbose=1,
    )
    
    model.fit(TX, Ty, sample_weight, Tqids, monitor=monitor)
    save_model(model, model_save_path)

    gen_info(model.estimators_)
    print 'output tree info at trees.info'
    
    print sum(model.feature_importances_)
    print model.feature_importances_
    print np.argsort(model.feature_importances_)
    
    id2featrue = {}
    for line in open('../dicts/featrue2id'):
        line = line.strip()
        featrue,id = line.split()
        id = int(id)
        id2featrue[id] = featrue
    for i in np.argsort(model.feature_importances_):
        print id2featrue[i],model.feature_importances_[i]
```

## è¯»å–æ•°æ®

`pyltr.data.letor.read_dataset`é€»è¾‘ï¼š

```c
/*
source: string or iterable å¯è¿­ä»£è¯»å–çš„æ•°æ®ã€‚å¯ä»¥æ˜¯æ‰“å¼€çš„æ–‡ä»¶æè¿°ç¬¦
has_targets: bool æ˜¯å¦å­˜åœ¨yï¼ˆå³æ ‡ç­¾ï¼‰,å¦‚æœæœ‰çš„åŒ–ï¼Œåˆ™å¿…é¡»æ”¾åˆ°æ¯ä¸€è¡Œçš„ç¬¬ä¸€ä¸ª
one_indexed: bool å‚æ•°xæ˜¯å¦æ˜¯æŒ‰ç…§ç´¢å¼•å¼€å§‹çš„ï¼ˆä¹Ÿå°±æ˜¯æ˜¯ä»1å¼€å§‹è¿˜æ˜¯ä»0å¼€å§‹ï¼‰
missing: float æŸä¸ªçº¬åº¦çš„æ•°æ®ç¼ºå¤±æ—¶çš„é»˜è®¤å€¼
has_weight: bool æ˜¯å¦æ¯ä¸ªæ•°æ®æœ‰ä¸åŒçš„æƒé‡ï¼Œå¦‚æœæœ‰çš„è¯ï¼Œæƒé‡å¿…é¡»åœ¨æ¯è¡Œçš„ç¬¬äºŒä¸ªå€¼
*/

def read_dataset(source, has_targets=True, one_indexed=True, missing=0.0, has_weight=False):
    """Parses a LETOR dataset from `source`.

    Parameters
    ----------
    source : string or iterable of lines
        String, file, or other file-like object to parse.
    has_targets : bool, optional
        See `iter_lines`.
    one_indexed : bool, optional
        See `iter_lines`.
    missing : float, optional
        See `iter_lines`.

    Returns
    -------
    X : array of arrays of floats
        Feature matrix (see `iter_lines`).
    y : array of floats
        Target vector (see `iter_lines`).
    qids : array of objects
        Query id vector (see `iter_lines`).
    comments : array of strs
        Comment vector (see `iter_lines`).

    """
    if isinstance(source, sklearn.externals.six.string_types):
        source = source.splitlines()

    max_width = 0
    xs, ys, weights, qids, comments = [], [], [], [], []
    // å¤„ç†æ•°æ®
    it = iter_lines(source, has_targets=has_targets,
                    one_indexed=one_indexed, missing=missing, has_weight=has_weight)
    # æ·»åŠ æ¯ä¸€è¡Œæ•°æ®åˆ°å„ç§æ•°ç»„ä¸­
    for x, y, weight, qid, comment in it:
        xs.append(x)
        ys.append(y)
        weights.append(weight)
        qids.append(qid)
        comments.append(comment)
        # è®°å½•æœ€å¤§çš„xçº¬åº¦
        max_width = max(max_width, len(x))
    # æ ¹æ®æœ€å¤§çš„xçº¬åº¦ï¼Œå¯¹æ•´ä½“çš„xè¿›è¡Œå¡«å……ï¼Œä¿è¯æ‰€æœ‰xçº¬åº¦ä¸€è‡´
    assert max_width > 0
    X = np.ndarray((len(xs), max_width), dtype=np.float64)
    X.fill(missing)
    for i, x in enumerate(xs):
        X[i, :len(x)] = x
    ys = np.array(ys) if has_targets else None
    qids = np.array(qids)
    comments = np.array(comments)
    weights = np.array(weights)

    return (X, ys, weights, qids, comments)
```



```python
# å‚æ•°ä¸ä¸Šè¿°çš„å‡½æ•°ä¸€è‡´
def iter_lines(lines, has_targets=True, one_indexed=True, missing=0.0, has_weight=False):
    """Transforms an iterator of lines to an iterator of LETOR rows.

    Each row is represented by a (x, y, qid, comment) tuple.

    Parameters
    ----------
    lines : iterable of lines
        Lines to parse.
    has_targets : bool, optional
        Whether the file contains targets. If True, will expect the first token
        of every line to be a real representing the sample's target (i.e.
        score). If False, will use -1 as a placeholder for all targets.
    one_indexed : bool, optional
        Whether feature ids are one-indexed. If True, will subtract 1 from each
        feature id.
    missing : float, optional
        Placeholder to use if a feature value is not provided for a sample.
    has_weight : bool, optional
        Whether has sample weight, If False, all samples' weight will be assigned 1.0, otherwise
        use the value read from data

    Yields
    ------
    x : array of floats
        Feature vector of the sample.
    y : float
        Target value (score) of the sample, or -1 if no target was parsed.
    weight : float
        sample  weight
    qid : object
        Query id of the sample. This is currently guaranteed to be a string.
    comment : str
        Comment accompanying the sample.

    """
    # éå†æ¯ä¸€è¡Œ
    for line in lines:
        # å»é™¤æœ«å°¾çš„ç©ºæ ¼ï¼Œå¹¶æŒ‰ç…§#åˆ‡å‰²ä¸ºä¸¤éƒ¨åˆ†ï¼Œdataä¸ºè¦è·å–çš„æ•°æ®ï¼Œcommentä¸ºæ³¨é‡Šï¼Œå³æ•°æ®å¯¹åº”çš„è¾…åŠ©ä¿¡æ¯
        data, _, comment = line.rstrip().partition('#')
        # æŒ‰ç…§ç©ºæ ¼åˆ‡å‰²æ•°æ®
        toks = data.split()

        num_features = 0
        # é»˜è®¤xå­˜åœ¨8ä¸ªçº¬åº¦ï¼Œå…ˆä½¿ç”¨missingå³ç¼ºå¤±å€¼è¿›è¡Œå¡«å……
        x = np.repeat(missing, 8)
        y = -1.0
        weight = 1.0
        # æœ‰æ ‡ç­¾yæ—¶ï¼Œç¬¬ä¸€ä¸ªå€¼ä¸ºæƒé‡
        if has_targets:
            y = float(toks[0])
            toks = toks[1:]
        # æœ‰æƒé‡æ—¶ï¼Œç¬¬äºŒä¸ªå€¼ä¸ºæƒé‡
        if has_weight:
            weight = float(toks[0])
            toks = toks[1:]
        # è·å–qidï¼Œä½¿ç”¨qid:åˆ‡å‰²
        qid = _parse_qid_tok(toks[0])
        # å‰©ä½™çš„toks[1:]å…¨éƒ¨ä¸ºxï¼Œå˜é‡æ¯ä¸€ä¸ªå‚æ•°
        for tok in toks[1:]:
            # è·å–å¯¹åº”çš„çº¬åº¦å’Œå€¼
            fid, _, val = tok.partition(':')
            fid = int(fid)
            val = float(val)
            # å¦‚æœä½¿ç”¨ç´¢å¼•å¼€å§‹ï¼Œåˆ™éœ€è¦å°†fidå‡å»1ï¼ˆå³æ–‡ä»¶ä¸­ä»1å¼€å§‹ï¼‰
            if one_indexed:
                fid -= 1
            assert fid >= 0
            # å¦‚æœå½“å‰xé•¿åº¦å°äºå…¶ç´¢å¼•ï¼Œåˆ™è¯´æ˜é¢„åˆ†é…çš„ç©ºé—´ä¸è¶³
            while len(x) <= fid:
                orig = len(x)
                # å°†xæ‰©å¤§åˆ°ä¸¤å€ï¼Œæ‰©å¤§éƒ¨åˆ†ä¹Ÿä½¿ç”¨missingå¡«å……
                x.resize(len(x) * 2)
                x[orig:orig * 2] = missing
            # è®¾ç½®å¯¹åº”xå‘é‡å€¼
            x[fid] = val
            # è®°å½•å®é™…xçš„çº¬åº¦
            num_features = max(fid + 1, num_features)
        # æ ¹æ®çº¬åº¦å¯¹Xè¿›è¡Œresize
        assert num_features > 0
        x.resize(num_features)

        yield (x, y, weight, qid, comment)
```

æ ¹æ®ä¸Šè¿°ä»£ç ï¼Œå¯ä»¥ç›´åˆ°ï¼Œpyltrè¯»å–æ•°æ®çš„æ ¼å¼å¦‚ä¸‹ï¼š

```c
y weight qid:value x_1:value_1 x_2:value_2 ... x_n:value_n#commment
```

å…¶ä¸­yå’Œweightéå¿…é¡»ï¼ˆè®­ç»ƒåº”è¯¥å¿…é¡»æœ‰yï¼‰qidçš„valueç”¨äºåˆ’åˆ†pairï¼Œx_iä¸ºè¾“å…¥çš„xå‘é‡ï¼Œå¦‚æœæŸä¸ªå‘é‡æ²¡æœ‰ï¼Œå¯ä»¥ä¸åœ¨æ–‡ä»¶ä¸­å­˜åœ¨ï¼Œä½¿ç”¨missingå¡«å……ã€‚commentæ˜¯è¿™æ¡æ•°æ®å¯¹åº”çš„é¢å¤–ä¿¡æ¯ï¼Œå¦‚queryå’Œå¯¹åº”çš„docã€‚

æ³¨æ„qidå¿…é¡»è¦æ˜¯åŒä¸€ä¸ªçš„è¿ç»­å­˜æ”¾ã€‚



# lambdaMart

## åˆå§‹åŒ–å‚æ•°

| å‚æ•°              | ç±»å‹                            | å«ä¹‰                                                         | é»˜è®¤å€¼ |
| ----------------- | ------------------------------- | ------------------------------------------------------------ | ------ |
| metric            | object                          | æ¨¡å‹è¦æœ€å¤§åŒ–çš„åº¦é‡ã€‚                                         |        |
| learning_rate     | float                           | å­¦ä¹ ç‡                                                       | 0.1    |
| n_estimators      | int                             | æå‡é˜¶æ®µæ•°é‡ï¼Œå³ç®€å•æ¨¡å‹çš„æ•°é‡ï¼ˆå†³ç­–æ ‘ï¼‰                     | 100    |
| max_depth         | int                             | å•ä¸ªå›å½’ä¼°è®¡å™¨çš„æœ€å¤§æ·±åº¦ã€‚æœ€å¤§æ·±åº¦é™åˆ¶äº†æ ‘ä¸­èŠ‚ç‚¹æ•°é‡ï¼Œé€šè¿‡è°ƒæ•´è¯¥å‚æ•°æ¥ä¼˜åŒ–æ€§èƒ½ã€‚å¦‚æœ`max_leaf_nodes`ä¸ä¸ºç©ºåˆ™å¿½ç•¥è¯¥å‚æ•°ã€‚ | 3      |
| min_samples_split | int                             | æ‹†åˆ†å†…éƒ¨èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°ã€‚                               | 2      |
| min_samples_leaf  | int                             | å¶èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°ã€‚                                     | 1      |
| subsample         | float                           | ç”¨äºæ‹Ÿåˆå•ä¸ªåŸºç¡€å­¦ä¹ å™¨çš„æ ·æœ¬æ¯”ä¾‹ã€‚                           | 1.0    |
| query_subsample   | float                           | ç”¨äºæ‹Ÿåˆå•ä¸ªåŸºç¡€å­¦ä¹ å™¨çš„æŸ¥è¯¢æ¯”ä¾‹ã€‚ï¼ˆå³qidæ•°é‡ï¼‰              | 1.0    |
| max_features      | intï¼Œfloatï¼Œstringï¼Œnone        | å†…éƒ¨èŠ‚ç‚¹ç”Ÿæˆå­èŠ‚ç‚¹æ—¶è€ƒè™‘çš„ç‰¹å¾æ•°é‡ã€‚ï¼ˆåœ¨å­˜åœ¨å¤§é‡ç‰¹å¾æ—¶åŠ é€Ÿï¼‰ã€‚intæ—¶ï¼Œè¡¨ç¤ºè¦è€ƒè™‘çš„æ•°é‡ã€‚floatæ—¶ï¼Œè¡¨ç¤ºæ‰€æœ‰èŠ‚ç‚¹çš„ç™¾åˆ†æ¯”ï¼Œautoæ—¶ä¸ºsqrt(n_features)ï¼Œsqrtå’Œautoä¸€æ ·ï¼Œlog2ä¸º($log_2{n\_features}$)ï¼Œnoneæ—¶ä¸ºæ‰€æœ‰ç‰¹å¾å‡è¦è€ƒè™‘ã€‚é€‰æ‹©â€œmax_features < n_featuresâ€ä¼šå¯¼è‡´æ–¹å·®å‡å°‘å’Œåå·®å¢åŠ ã€‚æ³¨æ„ï¼šåœ¨æ‰¾åˆ°è‡³å°‘ä¸€ä¸ªèŠ‚ç‚¹æ ·æœ¬çš„æœ‰æ•ˆåˆ†åŒºä¹‹å‰ï¼Œåˆ†å‰²çš„æœç´¢ä¸ä¼šåœæ­¢ï¼Œå³ä½¿å®ƒæ£€æŸ¥å·²ç»è¶…è¿‡ ``max_features`` ä¸ªç‰¹å¾ã€‚ | none   |
| max_leaf_nodes    | int/none                        | ç”Ÿæˆæ ‘çš„æœ€å¤§å¶èŠ‚ç‚¹æ•°é‡ã€‚å¦‚æœæ˜¯noneï¼Œåˆ™è¡¨ç¤ºæ— é™åˆ¶ã€‚ä¸ä¸ºnoneæ—¶å°†å¿½ç•¥å‚æ•°`max_depth` | none   |
| verbose           | int                             | å¯ç”¨è¯¦ç»†è¾“å‡ºã€‚ å¦‚æœä¸º 1ï¼Œåˆ™å®ƒä¼šä¸æ—¶æ‰“å°è¿›åº¦å’Œæ€§èƒ½ï¼ˆæ ‘è¶Šå¤šé¢‘ç‡è¶Šä½ï¼‰ã€‚ å¦‚æœå¤§äº 1ï¼Œåˆ™å®ƒä¼šæ‰“å°æ¯æ£µæ ‘çš„è¿›åº¦å’Œæ€§èƒ½ã€‚ | 0      |
| warm_start        | bool                            | å½“è®¾ç½®ä¸º ``True`` æ—¶ï¼Œé‡ç”¨ä¹‹å‰è°ƒç”¨ fit çš„é›†æˆæ¨¡å‹å¹¶å‘é›†æˆæ·»åŠ æ›´å¤šä¼°è®¡é‡ï¼Œå¦åˆ™ï¼Œåªéœ€åˆ é™¤ä¹‹å‰çš„è§£å†³æ–¹æ¡ˆã€‚ | false  |
| random_state      | intï¼ŒRandomState instanceï¼Œnone | å¦‚æœæ˜¯ intï¼Œrandom_state æ˜¯éšæœºæ•°ç”Ÿæˆå™¨ä½¿ç”¨çš„ç§å­ï¼› å¦‚æœæ˜¯ RandomState å®ä¾‹ï¼Œrandom_state æ˜¯éšæœºæ•°ç”Ÿæˆå™¨ï¼› å¦‚æœæ²¡æœ‰ï¼Œéšæœºæ•°ç”Ÿæˆå™¨æ˜¯ `np.random` ä½¿ç”¨çš„ RandomState å®ä¾‹ã€‚ | none   |

ä¸Šè¿°æ˜¯åˆå§‹åŒ–LambdaMARTçš„å‚æ•°ï¼Œç”¨äºå®šä¹‰å¦‚ä¸‹ç”Ÿæˆæ¨¡å‹ã€‚é™¤äº†è¿™äº›å‚æ•°ä»¥å¤–ï¼Œè¿˜åŒ…å«å¦‚ä¸‹çš„å†…éƒ¨å±æ€§ï¼š

| å±æ€§                 | ç±»å‹                                               | å«ä¹‰                                                         |
| -------------------- | -------------------------------------------------- | ------------------------------------------------------------ |
| feature_importances_ | n_featuresçº¬æ•°ç»„                                   | ç‰¹å¾é‡è¦æ€§ï¼ˆè¶Šé«˜ï¼Œç‰¹å¾è¶Šé‡è¦ï¼‰ã€‚                             |
| oob_improvement_     | n_estimatorsçº¬æ•°ç»„                                 | è¢‹å¤–æ ·æœ¬ç›¸å¯¹äºå‰ä¸€æ¬¡è¿­ä»£çš„æŸå¤±ï¼ˆ= åå·®ï¼‰æ”¹è¿›ã€‚``oob_improvement_[0]`` æ˜¯å¯¹ ``init`` ä¼°è®¡å™¨çš„ç¬¬ä¸€é˜¶æ®µæŸå¤±çš„æ”¹è¿›ã€‚ |
| train_score_         | n_estimatorsçº¬æ•°ç»„                                 | ç¬¬ i ä¸ªåˆ†æ•° ``train_score_[i]`` æ˜¯æ¨¡å‹åœ¨è¿­ä»£ ``i`` æ—¶åœ¨è¢‹å†…æ ·æœ¬ä¸Šçš„åå·®ï¼ˆ= æŸå¤±ï¼‰ã€‚å¦‚æœ ``subsample == 1`` è¿™å°±æ˜¯è®­ç»ƒæ•°æ®çš„åå·®ã€‚ |
| estimators_          | [n_estimators, 1]çš„ndarrayå­˜å‚¨è®­ç»ƒå®Œæˆçš„å†³ç­–æ ‘é›†åˆ | æ‹Ÿåˆå­ä¼°è®¡é‡çš„é›†åˆã€‚ å¯¹äºäºŒå…ƒåˆ†ç±»ï¼Œ``loss_.K`` ä¸º 1ï¼Œå¦åˆ™ä¸º n_classesã€‚ |
| estimators_fitted_   | int                                                | å®é™…æ‹Ÿåˆçš„å­ä¼°è®¡é‡çš„æ•°é‡ã€‚ è¿™å¯èƒ½ä¸ n_estimators åœ¨æå‰åœæ­¢ã€ä¿®å‰ªç­‰æƒ…å†µä¸‹ä¸åŒã€‚ |

### metric

è¿›è¡Œè®­ç»ƒæ•ˆæœçš„åº¦é‡ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯NDCGï¼Œå…¶å®šä¹‰å¦‚ä¸‹ï¼š

```python
class NDCG(Metric):
    def __init__(self, k=10, gain_type='exp2'):
        super(NDCG, self).__init__()
        self.k = k
        self.gain_type = gain_type
        self._dcg = DCG(k=k, gain_type=gain_type)
        self._ideals = {}
```

å…¶ä¸­kè¡¨ç¤ºåªè®¡ç®—æ’åºå‰kçš„æ•°æ®çš„NDCGã€‚`gain_type`è¡¨ç¤ºåº¦é‡NDCGæ˜¯ï¼Œæ¯ä¸ªæ–‡æ¡£çš„åŠ æƒå€¼ï¼Œå¯ä»¥é€‰æ‹©ï¼š

exp2è¡¨ç¤ºä½¿ç”¨çš„ç®—åˆ†å‡½æ•°ä¸ºï¼š
$$
DCG_P\sum_i^P\frac{2^{rel_i}-1}{log_2{(i+1)}}
$$
identityè¡¨ç¤ºä½¿ç”¨çš„å‡½æ•°ä¸ºï¼š
$$
DCG_P\sum_i^P\frac{x}{log_2{(i+1)}}
$$


## æ‰§è¡Œfitï¼ˆæ‹Ÿåˆï¼‰å‡½æ•°

```python

    def fit(self, X, y, sample_weight, qids, monitor=None):
        """Fit lambdamart onto a dataset.

        Parameters
        ----------

        X : array_like, shape = [n_samples, n_features]
            Training vectors, where n_samples is the number of samples
            and n_features is the number of features.
        y : array_like, shape = [n_samples]
            Target values (integers in classification, real numbers in
            regression)
            For classification, labels must correspond to classes.
        sample_weight : array_like, shape = [n_samples]
            The weight of the samples.
        qids : array_like, shape = [n_samples]
            Query ids for each sample. Samples must be grouped by query such
            that all queries with the same qid appear in one contiguous block.
        monitor : callable, optional
            The monitor is called after each iteration with the current
            iteration, a reference to the estimator and the local variables of
            ``_fit_stages`` as keyword arguments ``callable(i, self,
            locals())``. If the callable returns ``True`` the fitting procedure
            is stopped. The monitor can be used for various things such as
            computing held-out estimates, early stopping, model introspecting,
            and snapshoting.

        """
        // å¦‚æœåˆå§‹åŒ–æ—¶å‚æ•°warm_startä¸ºfalseï¼Œåˆ™å°†å…¶ä¹‹å‰å¯èƒ½å­˜åœ¨çš„è®­ç»ƒå±æ€§å…¨éƒ¨æ¸…ç©º
        if not self.warm_start:
            self._clear_state()
        
        // æ£€éªŒè¾“å…¥æ•°æ®
        X, y = sklearn.utils.check_X_y(X, y, dtype=sklearn.tree._tree.DTYPE)
        n_samples, self.n_features = X.shape

        sklearn.utils.check_consistent_length(X, y, qids)
        if y.dtype.kind == 'O':
            y = y.astype(np.float64)

        random_state = sklearn.utils.check_random_state(self.random_state)
        self._check_params()
        
        // åˆ†é…å†…éƒ¨å±æ€§ç©ºé—´ã€‚
        if not self._is_initialized():
            self._init_state()
            begin_at_stage = 0
            y_pred = np.zeros(y.shape[0])
        else:
            if self.n_estimators < self.estimators_.shape[0]:
                raise ValueError('n_estimators=%d must be larger or equal to '
                                 'estimators_.shape[0]=%d when '
                                 'warm_start==True'
                                 % (self.n_estimators,
                                    self.estimators_.shape[0]))
            begin_at_stage = self.estimators_.shape[0]
            self.estimators_fitted_ = begin_at_stage
            self.estimators_.resize((self.n_estimators, 1))
            self.train_score_.resize(self.n_estimators)
            if self.query_subsample < 1.0:
                self.oob_improvement_.resize(self.n_estimators)
            y_pred = self.predict(X)
        
        // æ‰§è¡Œfit
        n_stages = self._fit_stages(X, y, sample_weight, qids, y_pred,
                                    random_state, begin_at_stage, monitor)

        // å¦‚æœå®é™…ç”Ÿæˆçš„æ ‘æ•°é‡å°äºåˆå§‹åŒ–å‚æ•°n_estimatorsï¼Œåˆ™å°†ç›¸åº”é¢„åˆ†é…çš„å†…éƒ¨å±æ€§å˜æ›´ä¸ºå®é™…å¤§å°
        if n_stages < self.estimators_.shape[0]:
            self.trim(n_stages)

        return self
```



```python
    def _fit_stages(self, X, y, sample_weight, qids, y_pred, random_state,
                    begin_at_stage=0, monitor=None):
        // æ ·æœ¬æ•°é‡
        n_samples = X.shape[0]
        // æ¯æ¬¡è®­ç»ƒæ˜¯å¦ä½¿ç”¨å…¨éƒ¨æ ·æœ¬
        do_subsample = self.subsample < 1.0
        #sample_weight = np.ones(n_samples, dtype=np.float64)
        // è·å–qidæ•°é‡ï¼ˆuniqueï¼›å¤šå°‘ä¸ªqueryï¼‰
        n_queries = check_qids(qids)
        // æ ¹æ®qidç”Ÿæˆqueryç»„ï¼Œå…¶ä¸­å…ƒç´ ä¸ºqidï¼Œåœ¨æ ·æœ¬ä¸­èµ·å§‹ä¸‹æ ‡ï¼Œæœ«å°¾ä¸‹æ ‡ï¼Œä¸‹æ ‡åˆ—è¡¨
        query_groups = np.array([(qid, a, b, np.arange(a, b))
                                 for qid, a, b in get_groups(qids)],
                                dtype=np.object)
        // ä¹‹å‰è®¡ç®—çš„qidæ•°é‡åº”è¯¥å’Œqueryç»„æ•°é‡ä¸€è‡´
        assert n_queries == len(query_groups)
        // æ¯æ¬¡è®­ç»ƒæ˜¯å¦ä½¿ç”¨å…¨éƒ¨query
        do_query_oob = self.query_subsample < 1.0
        // queryçš„maskï¼Œå½“ä¸ä½¿ç”¨å…¨éƒ¨queryæ—¶ï¼Œä½¿ç”¨è¯¥æ•°ç»„å’Œquery_groupsè·å–æ‰€æœ‰è¦å‚åŠ è®­ç»ƒçš„æ ·æœ¬ï¼Œåˆå§‹åŒ–å…¨éƒ¨ä¸º1ï¼Œè¡¨ç¤ºæ‰€æœ‰queryå‡å‚ä¸è®­ç»ƒ
        query_mask = np.ones(n_queries, dtype=np.bool)
        // queryæ•°ç»„ä¸‹æ ‡ï¼Œåœ¨ä¸ä½¿ç”¨å…¨éƒ¨queyæ—¶ï¼Œéœ€è¦è¿›è¡Œæ‰“æ•£ï¼Œé€‰æ‹©éƒ¨åˆ†queryç»„
        query_idx = np.arange(n_queries)
        // é€‰æ‹©å…¨éƒ¨queryç»„ä¸­çš„æ¯”ä¾‹
        q_inbag = max(1, int(self.query_subsample * n_queries))

        // åˆå§‹åŒ–è¾“å‡º
        if self.verbose:
            verbose_reporter = _VerboseReporter(self.verbose)
            verbose_reporter.init(self, begin_at_stage)
        
        // æ‰§è¡Œæ¯ä¸€ä¸ªåŸºç¡€è®­ç»ƒ
        for i in range(begin_at_stage, self.n_estimators):
            // å¦‚æœä¸ä½¿ç”¨å…¨éƒ¨çš„queryç»„
            if do_query_oob:
                // å°†ç´¢å¼•éšæœºåŒ–
                random_state.shuffle(query_idx)
                // é‡æ–°åˆå§‹åŒ–queryçš„mask
                query_mask = np.zeros(n_queries, dtype=np.bool)
                // é€‰æ‹©æŒ‡å®šéƒ¨åˆ†çš„queryè¿›è¡Œè®­ç»ƒ
                query_mask[query_idx[:q_inbag]] = 1
            // å®é™…éœ€è¦å‚ä¸æœ¬è½®è®­ç»ƒçš„queryç»„
            query_groups_to_use = query_groups[query_mask]
            // å®é™…éœ€è¦å‚ä¸è®­ç»ƒçš„æ ·æœ¬mask
            sample_mask = np.zeros(n_samples, dtype=np.bool)
            // å˜é‡æ¯ä¸€ä¸ªqueryç»„
            for qid, a, b, sidx in query_groups_to_use:
                // éœ€è¦ä½¿ç”¨çš„æ ·æœ¬ä¸‹æ ‡
                sidx_to_use = sidx
                // å¦‚æœä¸ä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼Œä¸queryç±»ä¼¼ï¼Œå¯¹queryç»„ä¸­çš„æ ·æœ¬è¿›è¡Œéšæœºé‡‡æ ·ï¼Œä½†æ„Ÿè§‰è¿™é‡Œæœ‰é—®é¢˜ï¼Œå› ä¸ºå–çš„æœ€å¤§å€¼æ˜¯b-1ä¹˜ä»¥é‡‡æ ·ç³»æ•°ï¼Œä½†æ¯ä¸ªæ ·æœ¬ä¸­æ ·æœ¬æ•°é‡åº”è¯¥æ˜¯b-aï¼Œä½†æ˜¯å¦‚æœä½¿ç”¨b-aï¼Œä¹Ÿæ— æ³•ä¿è¯è®­ç»ƒä½¿ç”¨çš„æ ·æœ¬æ•°é‡æ˜¯æŒ‡å®šçš„æ¯”ä¾‹ï¼Œå› ä¸ºè¿˜æœ‰å¯èƒ½åªæ˜¯ä½¿ç”¨äº†éƒ¨åˆ†çš„queryã€‚
                if do_subsample:
                    query_samples_inbag = max(
                        1, int(self.subsample * (b - 1)))
                    random_state.shuffle(sidx)
                    sidx_to_use = sidx[:query_samples_inbag]
                sample_mask[sidx_to_use] = 1


            // è®°å½•æœªå‚ä¸è®­ç»ƒçš„æ ·æœ¬çš„å½“å‰è®­ç»ƒåˆ†æ•°
            if do_query_oob:
                old_oob_total_score = 0.0
                for qid, a, b, _ in query_groups[~query_mask]:
                    old_oob_total_score += self.metric.evaluate_preds(
                        qid, y[a:b], y_pred[a:b])

            // è¿›è¡Œè®­ç»ƒï¼Œå¹¶è·å–æ–°çš„æ ·æœ¬é¢„ä¼°
            y_pred = self._fit_stage(i, X, y, qids, y_pred, sample_weight,
                                     sample_mask, query_groups_to_use,
                                     random_state)
            
            // è®¡ç®—å‚ä¸è®­ç»ƒåï¼Œè®­ç»ƒæ ·æœ¬è¯„åˆ†å’Œæœªå‚ä¸è®­ç»ƒæ ·æœ¬è¯„åˆ†
            train_total_score, oob_total_score = 0.0, 0.0
            for qidx, (qid, a, b, _) in enumerate(query_groups):
                score = self.metric.evaluate_preds(
                    qid, y[a:b], y_pred[a:b])
                if query_mask[qidx]:
                    train_total_score += score
                else:
                    oob_total_score += score
            // è®°å½•å½“å‰å¾ªç¯çš„å‚ä¸è®­ç»ƒçš„æ ·æœ¬çš„åˆ†æ•°
            self.train_score_[i] = train_total_score / q_inbag
            // å¦‚æœä¸æ˜¯ä½¿ç”¨å…¨éƒ¨æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œåˆ™è®°å½•æœ¬è½®è®­ç»ƒå¯¹æœªå‚ä¸è®­ç»ƒæ ·æœ¬çš„åˆ†æ•°æå‡
            if do_query_oob:
                if q_inbag < n_queries:
                    self.oob_improvement_[i] = \
                        ((oob_total_score - old_oob_total_score) /
                         (n_queries - q_inbag))

            # åˆ¤æ–­æ˜¯å¦ç»ˆæ­¢è®­ç»ƒ
            early_stop = False
            monitor_output = None
            if monitor is not None:
                monitor_output = monitor(i, self, locals())
                if monitor_output is True:
                    early_stop = True
            # è¾“å‡ºå½“å‰è®­ç»ƒè¿›åº¦æ—¥å¿—`	1qnm  Zxdsdwertyul;xcv
            if self.verbose > 0:
                verbose_reporter.update(i, self, monitor_output)

            if early_stop:
                break

        return i + 1
```

æ‰§è¡Œçš„è®­ç»ƒå‡½æ•°

```python
    def _fit_stage(self, i, X, y, qids, y_pred, sample_weight, sample_mask,
                   query_groups, random_state):
        """Fit another tree to the boosting model."""
        assert sample_mask.dtype == np.bool

        n_samples = X.shape[0]

        all_lambdas = np.zeros(n_samples)
        all_deltas = np.zeros(n_samples)
        #æ ¹æ®queryç»„è®¡ç®—lambdaå’Œlambdaå¯¹æ¨¡å‹çš„æ¢¯åº¦
        for qid, a, b, _ in query_groups:
            lambdas, deltas = self._calc_lambdas_deltas(qid, y[a:b],
                                                        y_pred[a:b])
            all_lambdas[a:b] = lambdas
            all_deltas[a:b] = deltas
 
        # æ„å»ºå›å½’å†³ç­–æ ‘
        tree = sklearn.tree.DecisionTreeRegressor(
            criterion='friedman_mse',
            splitter='best',
            presort=True,
            max_depth=self.max_depth,
            min_samples_split=self.min_samples_split,
            min_samples_leaf=self.min_samples_leaf,
            min_weight_fraction_leaf=0.0,
            max_features=self.max_features,
            max_leaf_nodes=self.max_leaf_nodes,
            random_state=random_state)
        # æ ¹æ®é‡‡æ ·ï¼Œç¡®å®šæ¨¡å‹æƒé‡ï¼Œå¯¹äºæƒé‡ä¸º0çš„æ•°æ®ï¼Œç›¸å½“äºæ²¡æœ‰å‚ä¸è®­ç»ƒ
        if self.subsample < 1.0 or self.query_subsample < 1.0:
            sample_weight = sample_weight * sample_mask.astype(np.float64)
        # å›å½’æ ‘è®­ç»ƒï¼Œæ‹Ÿåˆæ–°çš„lambda
        tree.fit(X, all_lambdas, sample_weight=sample_weight,
                 check_input=False)
        # æ›´æ–°æ¨¡å‹
        self._update_terminal_regions(tree.tree_, X, y, all_lambdas,
                                      all_deltas, y_pred, sample_mask)
        self.estimators_[i, 0] = tree
        self.estimators_fitted_ = i + 1

#        print 'sample_weight:', sample_weight
        return y_pred
```

## è·å–lambdaåŠç›¸å¯¹äºå½“å‰æ¨¡å‹çš„æ¢¯åº¦

è¿™é‡Œè®¡ç®—ç®—æ³•ä»‹ç»ä¸­çš„$\lambda_i$å’Œ$w_i$ã€‚

å…¶æ‰§è¡Œé€»è¾‘ä¸ºï¼š

```python
    # è¾“å…¥ä¸ºqidï¼Œyä¸ºæ ·æœ¬çœŸå®labelï¼Œy_predä¸ºæ¨¡å‹è®­ç»ƒçš„è¾“å‡ºlabelï¼Œy, y_predå‡ä¸ºä¸€ç»´æ•°ç»„
    def _calc_lambdas_deltas(self, qid, y, y_pred):
        ns = y.shape[0]
        '''
        è·å–æŒ‰ç…§é¢„ä¼°çš„åˆ†æ•°æ¯ä¸ªæ–‡æ¡£çš„æ’åï¼ˆä»0å¼€å§‹ï¼‰
        def get_sorted_y_positions(y, y_pred, check=True):
            if check:
                y = sklearn.utils.validation.column_or_1d(y)
                y_pred = sklearn.utils.validation.column_or_1d(y_pred)
                sklearn.utils.validation.check_consistent_length(y, y_pred)
            # å¯¹ç´¢å¼•æŒ‰ç…§äºŒç»´æ•°ç»„æ’åºï¼Œå…ˆæŒ‰ç…§-y_predå†æŒ‰ç…§yæ’åº
            return np.lexsort((y, -y_pred))
        '''
        positions = get_sorted_y_positions(y, y_pred, check=False)
        # æŒ‰ç…§é¢„ä¼°çš„æ’åºå¯¹æ ·æœ¬çœŸå®çš„ç›¸å…³æ€§è¿›è¡Œé‡æ’
        actual = y[positions]
        # è·å–äº¤æ¢ä»»æ„ä¸¤ä¸ªæ–‡æ¡£å…¶ NDCGå·®å€¼
        swap_deltas = self.metric.calc_swap_deltas(qid, actual)
        # è·å–æ¨¡å‹å…³æ³¨çš„æ–‡æ¡£æ•°é‡ï¼ˆå³queryå¬å›çš„å‰kä¸ªï¼‰
        max_k = self.metric.max_k()
        # å¦‚æœæœªè®¾ç½®æ•°é‡ï¼Œæˆ–è€…æ•°é‡è¶…è¿‡äº†queryä¸‹å¬å›çš„æ–‡æ¡£æ•°é‡ï¼Œåˆ™è®¾ç½®ä¸ºå¬å›æ–‡æ¡£æ•°é‡
        if max_k is None or ns < max_k:
            max_k = ns

        lambdas = np.zeros(ns)
        deltas = np.zeros(ns)

        # éå†æ¯ä¸ªéœ€è¦è€ƒè™‘çš„æ’åºå‰Kä¸ªæ–‡æ¡£ï¼Œè®¡ç®—ä¸å…¶ä»–æ–‡æ¡£çš„lambdaå’Œæ¢¯åº¦
        for i in range(max_k):
            for j in range(i + 1, ns):
                # å¯¹äºç›¸å…³æ€§ä¸€è‡´åˆ™è·³è¿‡
                if actual[i] == actual[j]:
                    continue
                # è·å–æ–‡æ¡£ä½ç½®äº’æ¢çš„NDCGå·®å€¼ï¼Œè¿™é‡Œå¹¶æœªå–ç»å¯¹å€¼ï¼Œéœ€è¦åç»­åˆ¤æ–­
                delta_metric = swap_deltas[i, j]
                if delta_metric == 0.0:
                    continue
                // è·å–æ–‡æ¡£åœ¨æ•°æ®ä¸­å®é™…ä½ç½®
                a, b = positions[i], positions[j]
                # invariant: y_pred[a] >= y_pred[b]
                # å¯¹äºçœŸå®çš„ç›¸å…³æ€§ i < jå¤„ç†
                if actual[i] < actual[j]:
                    # è¿™æ—¶delta_metricåº”è¯¥å¤§äº0
                    assert delta_metric > 0.0
                    # è®¡ç®— 1/(1+e^(sj - si)),æ³¨æ„scipy.special.expit = 1/(1+exp(-x)),å³æœ¬èº«å­˜åœ¨ä¸€ä¸ªè´Ÿå·ï¼Œå› æ­¤è¾“å…¥æ—¶æ˜¯si-sj
                    logistic = scipy.special.expit(y_pred[a] - y_pred[b])
                    # è®¡ç®— 1/(1+e^(sj - si))* NDGCï¼Œå› ä¸ºdelta_metricæ˜¯æ­£æ•°ï¼Œå› æ­¤ç›´æ¥è®¡ç®—
                    l = logistic * delta_metric
                    # å¯¹äºaæ¥è¯´ï¼Œå…¶æ˜¯çœŸå®ç›¸å…³æ€§è¾ƒå·®çš„æ–‡æ¡£ï¼Œå› æ­¤å‡å»lambda
                    lambdas[a] -= l
                    # å¯¹åº”bæ¥è¯´ï¼Œå…¶æ˜¯çœŸå®ç›¸å…³æ€§è¾ƒå¥½çš„æ–‡æ¡£ï¼Œå› æ­¤åŠ ä¸Šlambda
                    lambdas[b] += l
                else:
                    assert delta_metric < 0.0
                    logistic = scipy.special.expit(y_pred[b] - y_pred[a])
                    l = logistic * -delta_metric
                    lambdas[a] += l
                    lambdas[b] -= l
                # è®¡ç®—æ¢¯åº¦
                gradient = (1 - logistic) * l
                # æ›´æ–°æ¢¯åº¦
                deltas[a] += gradient
                deltas[b] += gradient

        return lambdas, deltas
```

è¿™é‡Œçš„lambdaæ˜¯è´Ÿæ¢¯åº¦ï¼Œdeltasæ˜¯æ­£å¸¸çš„lambdaå¯¹$s_i$çš„å¯¼æ•°ã€‚

## è®¡ç®—äº’æ¢ä½ç½®åNDCGå€¼

```python
    def calc_swap_deltas(self, qid, targets):
        # æ ¹æ®qidè·å–IDCGå€¼
        ideal = self._get_ideal(qid, targets)
        if ideal < _EPS:
            return np.zeros((len(targets), len(targets)))
        # è®¡ç®—æ›¿æ¢åçš„NDCGå·®å€¼
        return self._dcg.calc_swap_deltas(
            qid, targets, coeff=1.0 / ideal)
      
      
      def _get_ideal(self, qid, targets):
        # å¦‚æœ_idealsä¸­å·²ç»å­˜åœ¨åˆ™ä¸ç”¨è®¡ç®—ç›´æ¥è¿”å›
        ideal = self._ideals.get(qid)
        if ideal is not None:
            return ideal
        # æŒ‰ç…§çœŸå®çš„æ–‡æ¡£ç›¸å…³æ€§å¯¹æ–‡æ¡£è¿›è¡Œæ’åºï¼ˆç”±å¤§åˆ°å°ï¼‰
        sorted_targets = np.sort(targets)[::-1]
        # è®¡ç®—IDCGå€¼
        ideal = self._dcg.evaluate(qid, sorted_targets)
        # å†™å…¥_ideals
        self._ideals[qid] = ideal
        return ideal
 
class DCG(Metric):
      def evaluate(self, qid, targets):
        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£å¾—åˆ†ï¼Œç›¸åŠ 
        # _gain_fnå¯é€‰ï¼Œç›´æ¥ä½¿ç”¨xè¿˜æ˜¯(2.0 ** x) - 1.0ï¼Œé€šè¿‡åˆå§‹åŒ–æ—¶æŒ‡å®šgain_fnå‚æ•°ï¼šidentityï¼šxï¼›exp2ï¼š(2.0 ** x) - 1.0
        # _get_discountä¸ºå¯¹åº”ä½ç½®çš„log(i+2)ï¼Œä½ç½®ä»0å¼€å§‹
        return sum(self._gain_fn(t) * self._get_discount(i)
                   for i, t in enumerate(targets) if i < self.k)
      
      def _get_discount(self, i):
        if i >= self.k:
            return 0.0
        while i >= len(self._discounts):
            self._grow_discounts()
        return self._discounts[i]
      
      def _grow_discounts(self):
        self._discounts = self._make_discounts(len(self._discounts) * 2)
        
      def _make_discounts(self, n):
        return np.array([1.0 / np.log2(i + 2.0) for i in range(n)])
      
      
      def calc_swap_deltas(self, qid, targets, coeff=1.0):
        n_targets = len(targets)
        deltas = np.zeros((n_targets, n_targets))
        # åªæŸ¥çœ‹å…³æ³¨çš„å‰kä¸ªæ–‡æ¡£ä¸å…¶ä»–æ–‡æ¡£äº¤æ¢é¡ºåºçš„å·®å€¼ï¼Œä½¿ç”¨ä¸Šæ–‡ä»‹ç»çš„æ–¹æ³•æ¥è®¡ç®—å·®å€¼ï¼Œè¿™é‡Œå¹¶æœªå–ç»å¯¹å€¼
        for i in range(min(n_targets, self.k)):
            for j in range(i + 1, n_targets):
                deltas[i, j] = coeff * \
                    (self._gain_fn(targets[i]) - self._gain_fn(targets[j])) * \
                    (self._get_discount(j) - self._get_discount(i))
```

## æ›´æ–°æ¨¡å‹

```python
    def _update_terminal_regions(self, tree, X, y, lambdas, deltas, y_pred,
                                 sample_mask):
        # è·å–æ¯ä¸ªæ ·æœ¬è¿›è¿‡å‰å‘ç®—æ³•æœ€ç»ˆè¾“å‡ºçš„å¶å­èŠ‚ç‚¹
        terminal_regions = tree.apply(X)
        masked_terminal_regions = terminal_regions.copy()
        # å¯¹äºæœªå‚ä¸è®­ç»ƒçš„æ ·æœ¬ï¼Œè®¾ç½®å…¶è¾“å‡ºçš„å¶å­èŠ‚ç‚¹ä¸º-1ï¼Œå³å‰”é™¤
        masked_terminal_regions[~sample_mask] = -1
        # éå†æ¨¡å‹çš„æ¯ä¸ªå¶å­èŠ‚ç‚¹
        for leaf in np.where(tree.children_left ==
                             sklearn.tree._tree.TREE_LEAF)[0]:
            # è·å–æ‰€æœ‰è¾“å‡ºä¸ºè¯¥èŠ‚ç‚¹çš„æ ·æœ¬
            terminal_region = np.where(masked_terminal_regions == leaf)
            # è®¡ç®—å…¶lambdaçš„å’Œ
            suml = np.sum(lambdas[terminal_region])
            # è®¡ç®—å…¶æ¢¯åº¦çš„å’Œ
            sumd = np.sum(deltas[terminal_region])
            # è®¾ç½®å¶å­èŠ‚ç‚¹çš„è¾“å‡ºå€¼
            tree.value[leaf, 0, 0] = 0.0 if abs(sumd) < 1e-300 else (suml / sumd)
        # æ›´æ–°æ¨¡å‹çš„é¢„æµ‹ç»“æœ
        y_pred += tree.value[terminal_regions, 0, 0] * self.learning_rate
```

æ³¨æ„ï¼Œè¿™é‡Œ

## é¢„æµ‹

è®­ç»ƒå®Œæˆåï¼Œæˆ–è€…éªŒè¯æ—¶ï¼Œæ‰§è¡Œé¢„æµ‹çš„å‡½æ•°å¦‚ä¸‹ï¼š

```python
def predict(self, X):
        # æ ¡éªŒè¾“å…¥
        X = sklearn.utils.validation.check_array(
            X, dtype=sklearn.tree._tree.DTYPE, order='C')
        score = np.zeros((X.shape[0], 1))
        # ç¡®å®šå®é™…å›å½’æ ‘æ•°é‡
        estimators = self.estimators_
        if self.estimators_fitted_ < len(estimators):
            estimators = estimators[:self.estimators_fitted_]
        # æ‰§è¡Œé¢„æµ‹
        sklearn.ensemble._gradient_boosting.predict_stages(
            estimators, X, self.learning_rate, score)

        return score.ravel()
```



## ç»ˆæ­¢è®­ç»ƒåˆ¤æ–­

```python
class ValidationMonitor(object):
    """Monitor for early stopping via validation set."""
    def __init__(self, X, y, qids, metric, stop_after=100,
                 trim_on_stop=True):
        if len(X) == 0:
            raise ValueError("A validation set can not be empty!")
        self.X, self.y = sklearn.utils.check_X_y(
            X, y, dtype=sklearn.tree._tree.DTYPE)
        self.qids = qids
        self.metric = metric
        self.stop_after = stop_after
        self.trim_on_stop = trim_on_stop

        sklearn.utils.check_consistent_length(self.X, self.y, self.qids)
        check_qids(qids)

        self._query_groups = list(get_groups(self.qids))
        self._y_pred = None
        self._prev_iter = -1
        self._iter_scores = []
        self._best_score = None
        self._best_score_i = None
```

å…¶ä¸­å‚æ•°`trim_on_stop`è¡¨ç¤ºæ˜¯å¦å…è®¸æå‰ç»ˆæ­¢ï¼Œ`stop_after`è¡¨ç¤ºå½“å‰è®­ç»ƒçš„è½®æ•°ï¼Œä¸æ•ˆæœæœ€å¥½çš„ä¸€è½®ä¹‹é—´å·®çš„è½®æ•°å¦‚æœè¶…è¿‡è¯¥å€¼åˆ™åœæ­¢è®­ç»ƒï¼ˆ`trim_on_stop`ä¸ºtrueæ—¶ï¼‰ã€‚

å…·ä½“ç»ˆæ­¢åˆ¤æ–­çš„é€»è¾‘å¦‚ä¸‹ï¼š

```python
    # è¿”å›trueè¡¨ç¤ºç»ˆæ­¢è®­ç»ƒ
    def __call__(self, i, model, localvars):
        """Returns True if the model should stop early.

        Otherwise, returns a status string.

        """
        # å½“å‰è½®æ•°
        assert i == self._prev_iter + 1
        self._prev_iter = i
        # å¦‚æœæ˜¯åŠ æ³•æ¨¡å‹ï¼Œåˆ™å¯¹é¢„æµ‹å€¼è¿›è¡Œç´¯åŠ 
        if isinstance(model, AdditiveModel):
            if self._y_pred is None:
                self._y_pred = model.predict(self.X)
            else:
                self._y_pred += model.iter_y_delta(i, self.X)
            y_pred = self._y_pred
        else:
            y_pred = model.predict(self.X)
        # è®¡ç®—æ¯ä¸ªqueryä¸‹æ–‡æ¡£çš„NDCGå€¼ï¼Œå¹¶åŠ å’Œ
        score = 0.0
        for qid, a, b in self._query_groups:
            sorted_y = get_sorted_y(self.y[a:b], y_pred[a:b])
            score += self.metric.evaluate(qid, sorted_y)
        # è¾“å‡ºå½“å‰è½®æ€»çš„NDCGå€¼
        print "monitor score: ", score
        # è®¡ç®—å¹³å‡æ¯ä¸ªqueryçš„NDCGå€¼
        score /= len(self._query_groups)
        # è®°å½•æœ€å¥½çš„è½®æ¬¡å’Œå…¶åˆ†æ•°
        if self._best_score is None or score > self._best_score:
            self._best_score = score
            self._best_score_i = i
        # è®°å½•å½“å‰è½®æ¬¡ä¸æœ€å¥½è½®æ¬¡ä¹‹é—´å·®è·
        since = i - self._best_score_i
        # å¦‚æœå…è®¸æå‰ç»ˆæ­¢ï¼Œå¹¶ä¸”è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶ï¼Œåˆ™ç»ˆæ­¢
        if self.trim_on_stop and \
                (since >= self.stop_after or i + 1 == model.n_estimators):
            # å°†æ¨¡å‹æŒ‰ç…§å®é™…å›å½’æ ‘æ•°é‡ç¼©å°
            model.trim(self._best_score_i + 1)
            return True
        # å¦‚æœæœªç»ˆæ­¢ï¼Œåˆ™è¿”å›å½“å‰è®­ç»ƒçŠ¶æ€ Cä¸ºqueryçš„å¹³å‡NDCGï¼ŒBä¸ºæœ€å¥½çš„åˆ†æ•°ï¼ŒSä¸ºä¸¾ä¾‹ä¹‹å‰çš„æœ€å¥½åˆ†æ•°çš„è½®æ¬¡
        return 'C:{:12.4f} B:{:12.4f} S:{:3d}'.format(
            score, self._best_score, since)
```

## æ—¥å¿—è¾“å‡º

æ—¥å¿—è¾“å‡ºç±»å¦‚ä¸‹ï¼š

```python
class _VerboseReporter(object):
    """Reports verbose output to stdout.

    If ``verbose==1`` output is printed once in a while (when iteration mod
    verbose_mod is zero).; if larger than 1 then output is printed for
    each update.

    """
    def __init__(self, verbose):
        self.verbose = verbose

    def init(self, est, begin_at_stage=0):
        # header fields and line format str
        header_fields = ['Iter', 'Train score']
        verbose_fmt = ['{iter:>5d}', '{train_score:>12.4f}']
        # do oob?
        if est.query_subsample < 1:
            header_fields.append('OOB Improve')
            verbose_fmt.append('{oob_impr:>12.4f}')
        header_fields.append('Remaining')
        verbose_fmt.append('{remaining_time:>12s}')
        header_fields.append('Monitor Output')
        verbose_fmt.append('{monitor_output:>40s}')

        # print the header line
        print(('%5s ' + '%12s ' *
               (len(header_fields) - 2) + '%40s ') % tuple(header_fields))

        self.verbose_fmt = ' '.join(verbose_fmt)
        # plot verbose info each time i % verbose_mod == 0
        self.verbose_mod = 1
        self.start_time = time.time()
        self.begin_at_stage = begin_at_stage

    def update(self, j, est, monitor_output):
        """Update reporter with new iteration. """
        if monitor_output is True:
            print('Early termination at iteration ', j)
            return
        do_query_oob = est.query_subsample < 1
        # we need to take into account if we fit additional estimators.
        i = j - self.begin_at_stage  # iteration relative to the start iter
        if self.verbose > 1 or (i + 1) % self.verbose_mod == 0:
            oob_impr = est.oob_improvement_[j] if do_query_oob else 0
            remaining_time = ((est.n_estimators - (j + 1)) *
                              (time.time() - self.start_time) / float(i + 1))
            if remaining_time > 60:
                remaining_time = '{0:.2f}m'.format(remaining_time / 60.0)
            else:
                remaining_time = '{0:.2f}s'.format(remaining_time)
            if monitor_output is None:
                monitor_output = ''
            print(self.verbose_fmt.format(iter=j + 1,
                                          train_score=est.train_score_[j],
                                          oob_impr=oob_impr,
                                          remaining_time=remaining_time,
                                          monitor_output=monitor_output))
            if i + 1 >= 10:
                self.verbose_mod = 5
            if i + 1 >= 50:
                self.verbose_mod = 10
            if i + 1 >= 100:
                self.verbose_mod = 20
            if i + 1 >= 500:
                self.verbose_mod = 50
            if i + 1 >= 1000:
                self.verbose_mod = 100
```

è¾“å‡ºä¿¡æ¯å¦‚ä¸‹ï¼š

[![5qj7Z9.png](https://z3.ax1x.com/2021/10/28/5qj7Z9.png)](https://imgtu.com/i/5qj7Z9)

å…¶ä¸­è¾“å‡ºçš„æ¯ä¸ªå­—æ®µå«ä¹‰å¦‚ä¸‹ï¼š

| å­—æ®µ        | å«ä¹‰                                                         |
| ----------- | ------------------------------------------------------------ |
| Iter        | è¿­ä»£è½®æ•°                                                     |
| train score | å¹³å‡çš„NDCGåˆ†æ•°ï¼ˆè¶Šé«˜çº¦å¥½ï¼‰                                   |
| OOB Improve | æ¯æ¬¡ä¸ä¸Šä¸€æ­¥è¿­ä»£æå‡çš„train_scoreï¼Œå³delta train_scoreï¼›     |
| Remaining   | æ ¹æ®å½“å‰è®­ç»ƒè½®æ•°å’ŒèŠ±è´¹æ—¶é—´é¢„ä¼°è®­ç»ƒå®Œæˆæ—¶é—´                   |
| C           | éªŒè¯é›†çš„å¹³å‡NDGC                                             |
| B           | å†æ¬¡è¿­ä»£ä¸­éªŒè¯é›†ä¸­æœ€é«˜çš„å¹³å‡NDGC                             |
| S           | è¿­è¾¾åˆ°å½“å‰æ­¥æ•°ï¼Œè·ç¦»éªŒè¯é›†æœ€é«˜çš„å¹³å‡ndgcè¿­ä»£æ­¥æ•°çš„å·®(i-best_socre_i) |

# å­˜å‚¨æ¨¡å‹å¹¶è¾“å‡ºç›¸å…³æ•ˆæœ

ä½¿ç”¨pickleåŒ…å­˜å‚¨æ¨¡å‹ã€‚

```python
def save_model(model, file):
    save_file = open(file, "wb")
    pickle.dump(model, save_file)
    save_file.close()
```

è¾“å‡ºæ¯ä¸ªå› å­çš„é‡è¦æ€§ã€‚

```python
id2featrue = {}
    for line in open('../dicts/featrue2id'):
        line = line.strip()
        featrue,id = line.split()
        id = int(id)
        id2featrue[id] = featrue
    for i in np.argsort(model.feature_importances_):
        print id2featrue[i],model.feature_importances_[i]
```

å…¶ä¸­ä¸­å­˜å‚¨äº†featureidåˆ°featurenameçš„æ˜ å°„å…³ç³»ï¼š

```
ctr_norm 0
title_oral_tight_term_hit_count_norm 1
cqr_norm 2
pcqr 3
likeRank 4
title_oral_offset_value 5
title_oral_boost_term_hit_count 6
title_oral_offset_value_norm 7
tag_match_score 8
cqr_ 9
title_oral_levenshtein_distance_norm 10
```

# æ¨¡å‹è½¬æ¢

gbrankåªæ”¯æŒgbrankæ¨¡å‹ï¼Œå› æ­¤éœ€è¦å°†è®­ç»ƒå®Œæˆçš„lambdaMARTæ¨¡å‹è¿›è¡Œä¸€ä¸‹è½¬æ¢ã€‚

ä¿®æ”¹`notes/bin/trans_model_format.sh`ä¸­çš„è¾“å…¥å’Œè¾“å‡ºæ¨¡å‹å³å¯ã€‚

æ‰§è¡Œå¯¹åº”è„šæœ¬å³è·å¾—å¯ä»¥ç›´æ¥åœ¨vsrankä¸­ä½¿ç”¨çš„æ¨¡å‹ã€‚
